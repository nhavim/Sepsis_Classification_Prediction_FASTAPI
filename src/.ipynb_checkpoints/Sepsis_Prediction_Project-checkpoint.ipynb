{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76cddbf",
   "metadata": {},
   "source": [
    "# Exploring Patterns and Predictors in Patient Data to Uncover the Secrets of Sepsis Occurance. \n",
    "\n",
    "# Intro\n",
    "\n",
    "## General\n",
    "\n",
    "In the realm of healthcare, understanding the complex dynamics behind the occurrence of life-threatening conditions is of paramount importance. Sepsis, a potentially fatal condition resulting from the body's extreme response to an infection, remains a major challenge for healthcare providers worldwide. Unraveling the secrets of sepsis occurrence can lead to improved early detection, timely interventions, and ultimately, better patient outcomes.\n",
    "\n",
    "This project aims to delve into the vast pool of patient data, harnessing the power of data analysis and machine learning, to explore patterns and predictors associated with sepsis occurrence. By leveraging advanced computational techniques and drawing insights from comprehensive patient records, this research endeavor seeks to uncover hidden correlations, risk factors, and potential early warning signs that can facilitate earlier diagnosis and intervention.\n",
    "\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "##### - Hypothesis 1: Higher plasma glucose levels (PRG) are associated with an increased risk of developing sepsis.\n",
    "\n",
    "- Null Hypothesis: There is no association between higher plasma glucose levels (PRG) and the risk of developing sepsis.\n",
    "\n",
    "- Alternate Hypothesis: Higher plasma glucose levels (PRG) are associated with an increased risk of developing sepsis.\n",
    "\n",
    "Justification: Elevated glucose levels have been linked to impaired immune function and increased susceptibility to infections, including sepsis.\n",
    "\n",
    "\n",
    "##### -  Hypothesis 2: Abnormal blood work results, such as high values of PL, SK, and BD2, are indicative of a higher likelihood of sepsis.\n",
    "\n",
    "- Null Hypothesis: There is no association between abnormal blood work results, such as high values of PL, SK, and BD2, and the likelihood of sepsis.\n",
    "\n",
    "- Alternate Hypothesis: Abnormal blood work results, such as high values of PL, SK, and BD2, are indicative of a higher likelihood of sepsis.\n",
    "\n",
    "Justification: Abnormal blood work results may indicate an ongoing infection or an inflammatory response, which are key factors in sepsis development.\n",
    "\n",
    "##### -  Hypothesis 3:  Older patients are more likely to develop sepsis compared to younger patients.\n",
    "\n",
    "- Null Hypothesis: There is no difference in the likelihood of developing sepsis between older and younger patients.\n",
    "\n",
    "- Alternate Hypothesis: Older patients are more likely to develop sepsis compared to younger patients.\n",
    "\n",
    "Justification: Advanced age is a known risk factor for sepsis, as the immune system weakens with age and may be less able to mount an effective response to infections.\n",
    "\n",
    "##### -  Hypothesis 4: Patients with higher body mass index (BMI) values (M11) have a lower risk of sepsis.\n",
    "\n",
    "- Null Hypothesis: There is no association between body mass index (BMI) values (M11) and the risk of sepsis.\n",
    "\n",
    "- Alternate Hypothesis: Patients with higher body mass index (BMI) values (M11) have a lower risk of sepsis.\n",
    "\n",
    "Justification: Obesity has been associated with a dampened immune response, potentially leading to a decreased risk of developing sepsis.\n",
    "\n",
    "##### -  Hypothesis 5: Patients without valid insurance cards are more likely to develop sepsis.\n",
    "\n",
    "- Null Hypothesis: There is no association between the absence of valid insurance cards and the likelihood of developing sepsis.\n",
    "\n",
    "- Alternate Hypothesis: Patients without valid insurance cards are more likely to develop sepsis.\n",
    "\n",
    "Justification: Lack of access to healthcare, as indicated by the absence of valid insurance, may delay or hinder early detection and treatment of infections, potentially increasing the risk of sepsis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e942fae",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce0c43",
   "metadata": {},
   "source": [
    "## Importation\n",
    "\n",
    "Here is the section to import all the packages/libraries that will be used through this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Data Splitting and Models\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, roc_auc_score\n",
    "import pickle\n",
    "\n",
    "# Saving Model\n",
    "from joblib import dump\n",
    "\n",
    "# Others\n",
    "import shap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Set seaborn defaults and suppress warnings\n",
    "sns.set()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3675b4e",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "Here is the section to load the datasets (train, eval, test) and the additional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/aliduabubakari/Sepsis-Classification-with-FastAPI/main/Data/Paitients_Files_Train.csv\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/aliduabubakari/Sepsis-Classification-with-FastAPI/main/Data/Paitients_Files_Test.csv\"\n",
    "}\n",
    "\n",
    "train = pd.read_csv(urls[\"train\"])\n",
    "test = pd.read_csv(urls[\"test\"])\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b987b1",
   "metadata": {},
   "source": [
    "- ID: Unique number to represent patient ID\n",
    "\n",
    "- PRG: Plasma glucose\n",
    "\n",
    "- PL: Blood Work Result-1 (mu U/ml)\n",
    "\n",
    "- PR: Blood Pressure (mm Hg)\n",
    "\n",
    "- SK: Blood Work Result-2 (mm)\n",
    "\n",
    "- TS: Blood Work Result-3 (mu U/ml)\n",
    "\n",
    "- M11: Body mass index (weight in kg/(height in m)^2\n",
    "\n",
    "- BD2: Blood Work Result-4 (mu U/ml)\n",
    "\n",
    "- Age\t: patients age (years)\n",
    "\n",
    "- Insurance: If a patient holds a valid insurance card\n",
    "\n",
    "- Sepssis; Positive: if a patient in ICU will develop a sepsis , and Negative: otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac8b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cedeb1",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: EDA\n",
    "\n",
    "Here is the section to **inspect** the datasets in depth, **present** it, make **hypotheses** and **think** the *cleaning, processing and features creation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c920b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataset dimension \n",
    "print(\"Number of rows for Train data:\", train.shape[0])\n",
    "print(\"Number of columns for Train data:\", train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataset dimension \n",
    "print(\"Number of rows for Test Data:\", test.shape[0])\n",
    "print(\"Number of columns for Test Data:\", train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f2dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the summary statistics of numerical columns:\n",
    "train.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada8f9e",
   "metadata": {},
   "source": [
    "###### PRG\n",
    "\n",
    "- Count: 599\n",
    "- Mean: 3.82\n",
    "- Standard Deviation: 3.36\n",
    "- Range: 0 to 17\n",
    "\n",
    "###### PL\n",
    "\n",
    "- Count: 599\n",
    "- Mean: 120.15\n",
    "- Standard Deviation: 32.68\n",
    "- Range: 0 to 198\n",
    "\n",
    "###### PR\n",
    "\n",
    "- Count: 599\n",
    "- Mean: 68.73\n",
    "- Standard Deviation: 19.34\n",
    "- Range: 0 to 122\n",
    "\n",
    "###### SK\n",
    "\n",
    "- Count: 599\n",
    "- Mean: 20.56\n",
    "- Standard Deviation: 16.02\n",
    "- Range: 0 to 99\n",
    "\n",
    "###### TS\n",
    "\n",
    "- Count: 599\n",
    "- Mean: 79.46\n",
    "- Standard Deviation: 116.58\n",
    "- Range: 0 to 846\n",
    "\n",
    "###### M11\n",
    "\n",
    "- Count: 599\n",
    "- Mean: 31.92\n",
    "- Standard Deviation: 8.01\n",
    "- Range: 0 to 67.1\n",
    "\n",
    "###### BD2\n",
    "\n",
    "- Count: 599\n",
    "- Mean: 0.48\n",
    "- Standard Deviation: 0.34\n",
    "- Range: 0.078 to 2.42\n",
    "\n",
    "###### Age\n",
    "\n",
    "- Count: 599\n",
    "- Mean: 33.29\n",
    "- Standard Deviation: 11.83\n",
    "- Range: 21 to 81\n",
    "\n",
    "###### Insurance\n",
    "\n",
    "- Count: 599\n",
    "- Proportion with Insurance: 0.686 (68.6%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the summary statistics of numerical columns:\n",
    "test.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b43eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57956ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23696e25",
   "metadata": {},
   "source": [
    "## Univariate Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a06b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select columns to plot\n",
    "cols_to_plot = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2', 'Age']\n",
    "\n",
    "# Plot KDEs(kernel density estimation) for all columns\n",
    "fig, axes = plt.subplots(nrows=len(cols_to_plot), figsize=(8, 40))\n",
    "for i, col in enumerate(cols_to_plot):\n",
    "    sns.kdeplot(data=df, x=col, ax=axes[i], fill=True)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    \n",
    "    # Calculate mean, skewness, and kurtosis\n",
    "    mean_val = df[col].mean()\n",
    "    skewness_val = df[col].skew()\n",
    "    kurtosis_val = df[col].kurtosis()\n",
    "    \n",
    "    # Add mean, skewness, and kurtosis as text annotations\n",
    "    axes[i].text(0.6, 0.9, f'Mean: {mean_val:.2f}', transform=axes[i].transAxes)\n",
    "    axes[i].text(0.6, 0.8, f'Skewness: {skewness_val:.2f}', transform=axes[i].transAxes)\n",
    "    axes[i].text(0.6, 0.7, f'Kurtosis: {kurtosis_val:.2f}', transform=axes[i].transAxes)\n",
    "    \n",
    "    # Add mean line\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', label='Mean')\n",
    "    \n",
    "    # Add red dots to indicate potential outliers\n",
    "    outliers = df[(df[col] > mean_val + 3 * df[col].std()) | (df[col] < mean_val - 3 * df[col].std())]\n",
    "    axes[i].plot(outliers[col], [0] * len(outliers), 'ro', label='Potential Outliers')\n",
    "    \n",
    "    # Add legend\n",
    "    axes[i].legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571cc05",
   "metadata": {},
   "source": [
    "Based on the KDE plot analysis of the PRG variable, it appears that the distribution is positively skewed, suggesting the presence of some higher values. The distribution is also platykurtic, indicating a flatter peak and lighter tails compared to a normal distribution.\n",
    "\n",
    "Based on the KDE plot analysis of the PL variable, it appears that the distribution is approximately symmetric, with a mean value of 120.15. The distribution is mesokurtic, suggesting a similar shape to a normal distribution.\n",
    "\n",
    "The kde plot suggests that the blood pressure distribution is negatively skewed and has a more peaked shape with possible outliers.\n",
    "\n",
    "The kde plot suggests that the distribution of blood work result 2 is slightly positively skewed and has a flatter shape.This suggests that the distribution has fewer outliers or extreme values.\n",
    "\n",
    "The kde plot suggests that the distribution of TS (blood work result 3) is positively skewed and has a more peaked shape with heavier tails.This means that the tail of the distribution is extended to the right, indicating a higher frequency of lower values compared to higher values.This suggests that the distribution has more outliers or extreme values.\n",
    "\n",
    "The kde plot suggests that the distribution of body mass index is slightly negatively skewed and has a more peaked shape with heavier tails.This means that the tail of the distribution is extended to the left, indicating a higher frequency of higher values compared to lower values. This suggests that the distribution has more outliers or extreme values.\n",
    "The kde plot indicates a positively skewed distribution for the blood work result with a more peaked shape and heavier tails.This indicates a higher frequency of extreme values or outliers.This means that the tail of the distribution is extended to the right, suggesting a higher frequency of lower values compared to higher values.\n",
    "\n",
    "The kde plot indicates a positively skewed distribution of age, with a higher frequency of younger individuals. This indicates a more uniform spread of values without significant outliers or extreme values.This suggests that the tail of the distribution is extended to the right, indicating a higher frequency of younger individuals compared to older individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038800f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Count plot for \"Insurance\"\n",
    "sns.countplot(data=df, x='Insurance')\n",
    "\n",
    "# Set labels\n",
    "plt.xlabel('Insurance')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Set title\n",
    "plt.title('Distribution of Insurance')\n",
    "\n",
    "# Calculate percentage distribution\n",
    "total = len(df['Insurance'])\n",
    "percentages = df['Insurance'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Add data labels and percentage annotations\n",
    "for p, percentage in zip(plt.gca().patches, percentages):\n",
    "    count = p.get_height()\n",
    "    percentage_label = f'{percentage:.1f}%'\n",
    "    plt.gca().annotate(f'{count}\\n{percentage_label}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d806aa",
   "metadata": {},
   "source": [
    "##### Outlier Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7e0fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical_cols = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2', 'Age', 'Insurance']\n",
    "\n",
    "# Iterate over each numerical column\n",
    "for col in numerical_cols:\n",
    "    # Create a box plot\n",
    "    plt.boxplot(df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.ylabel(col)\n",
    "\n",
    "    # Get the outliers\n",
    "    outliers = df[df[col] > df[col].quantile(0.75) + 1.5 * (df[col].quantile(0.75) - df[col].quantile(0.25))]  # Adjust the outlier threshold if needed\n",
    "\n",
    "    # Print the number of outliers\n",
    "    num_outliers = len(outliers)\n",
    "    print(f\"Number of outliers in {col}: {num_outliers}\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69094465",
   "metadata": {},
   "source": [
    "## Bivariate Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd6252",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Numerical Variables - Violin plots with statistics\n",
    "numerical_vars = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2', 'Age']\n",
    "for var in numerical_vars:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(data=df, x='Sepssis', y=var)\n",
    "    plt.xlabel('Sepssis')\n",
    "    plt.ylabel(var)\n",
    "    plt.title(f'{var} Distribution by Sepssis')\n",
    "    \n",
    "    # Calculate statistics\n",
    "    positive_vals = df[df['Sepssis'] == 'Positive'][var]\n",
    "    negative_vals = df[df['Sepssis'] == 'Negative'][var]\n",
    "    stat_dict = {\n",
    "        'Positive': {\n",
    "            'Mean': np.mean(positive_vals),\n",
    "            'Median': np.median(positive_vals),\n",
    "            '25th Percentile': np.percentile(positive_vals, 25),\n",
    "            '75th Percentile': np.percentile(positive_vals, 75)\n",
    "        },\n",
    "        'Negative': {\n",
    "            'Mean': np.mean(negative_vals),\n",
    "            'Median': np.median(negative_vals),\n",
    "            '25th Percentile': np.percentile(negative_vals, 25),\n",
    "            '75th Percentile': np.percentile(negative_vals, 75)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add statistics as text annotations\n",
    "    plt.text(0.30, 0.8, f\"Positive:\\nMean: {stat_dict['Positive']['Mean']:.2f}\\nMedian: {stat_dict['Positive']['Median']:.2f}\\n25th Percentile: {stat_dict['Positive']['25th Percentile']:.2f}\\n75th Percentile: {stat_dict['Positive']['75th Percentile']:.2f}\", transform=plt.gca().transAxes)\n",
    "    plt.text(0.70, 0.8, f\"Negative:\\nMean: {stat_dict['Negative']['Mean']:.2f}\\nMedian: {stat_dict['Negative']['Median']:.2f}\\n25th Percentile: {stat_dict['Negative']['25th Percentile']:.2f}\\n75th Percentile: {stat_dict['Negative']['75th Percentile']:.2f}\", transform=plt.gca().transAxes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067957c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Variables - Bar plots\n",
    "categorical_vars = ['Insurance']\n",
    "for var in categorical_vars:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(data=df, x=var, hue='Sepssis')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'{var} Distribution by Sepssis')\n",
    "\n",
    "    # Calculate percentage distribution\n",
    "    total = len(df['Sepssis'])\n",
    "    percentages = df['Sepssis'].value_counts(normalize=True) * 100\n",
    "\n",
    "    # Add data labels and percentage annotations\n",
    "    for p, percentage in zip(plt.gca().patches, percentages):\n",
    "        count = p.get_height()\n",
    "        percentage_label = f'{percentage:.1f}%'\n",
    "        plt.gca().annotate(f'{count}\\n{percentage_label}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74efa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the target variable\n",
    "df['Sepssis_Encoded'] = df['Sepssis'].map({'Negative': 0, 'Positive': 1})\n",
    "\n",
    "# Calculate correlation coefficients\n",
    "numerical_vars = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2', 'Age', 'Insurance']\n",
    "correlations = df[numerical_vars + ['Sepssis_Encoded']].corr()\n",
    "\n",
    "# Print correlation coefficients\n",
    "for var in numerical_vars:\n",
    "    correlation = correlations.loc[var, 'Sepssis_Encoded']\n",
    "    print(f\"Pearson correlation between 'Sepssis_Encoded' and '{var}': {correlation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2', 'Age', 'Insurance']\n",
    "correlations = df[numerical_vars + ['Sepssis_Encoded']].corr()\n",
    "\n",
    "# Get correlation values\n",
    "correlation_values = correlations.loc[numerical_vars, 'Sepssis_Encoded']\n",
    "\n",
    "# Create bar plot using Seaborn\n",
    "sns.barplot(x=numerical_vars, y=correlation_values)\n",
    "\n",
    "# Add data labels to the bar plot\n",
    "for i, val in enumerate(correlation_values):\n",
    "    plt.text(i, val, round(val, 2), ha='center', va='bottom')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title('Correlation between Variables and Sepssis_Encoded')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0066c",
   "metadata": {},
   "source": [
    "Let's analyze the correlations between the 'Sepssis_Encoded' variable and each of the other variables:\n",
    "\n",
    "1. 'PRG': The correlation coefficient of 0.21 suggests a weak positive correlation between plasma glucose levels and the likelihood of developing sepsis. However, the correlation is not very strong.\n",
    "\n",
    "2. 'PL': The correlation coefficient of 0.45 indicates a moderate positive correlation between attribute 2 (blood work result-1) and the likelihood of developing sepsis. This suggests that higher values of PL are associated with a higher likelihood of sepsis.\n",
    "\n",
    "3. 'PR': The correlation coefficient of 0.06 indicates a very weak positive correlation between blood pressure and the likelihood of developing sepsis. The correlation is close to zero, suggesting that there is no meaningful relationship between these variables.\n",
    "\n",
    "4. 'SK': The correlation coefficient of 0.08 suggests a very weak positive correlation between attribute 4 (blood work result-2) and the likelihood of developing sepsis. The correlation is close to zero, indicating no significant relationship.\n",
    "\n",
    "5. 'TS': The correlation coefficient of 0.15 indicates a weak positive correlation between attribute 5 (blood work result-3) and the likelihood of developing sepsis. The correlation is not very strong, suggesting a limited relationship.\n",
    "\n",
    "6. 'M11': The correlation coefficient of 0.32 indicates a moderate positive correlation between body mass index (BMI) and the likelihood of developing sepsis. This suggests that higher BMI values are associated with a higher likelihood of sepsis.\n",
    "\n",
    "7. 'BD2': The correlation coefficient of 0.18 suggests a weak positive correlation between attribute 7 (blood work result-4) and the likelihood of developing sepsis. The correlation is not very strong, indicating a limited relationship.\n",
    "\n",
    "8. 'Age': The correlation coefficient of 0.21 suggests a weak positive correlation between age and the likelihood of developing sepsis. This implies that older patients may have a slightly higher likelihood of sepsis.\n",
    "\n",
    "9. 'Insurance': The correlation coefficient of 0.06 indicates a very weak positive correlation between insurance status and the likelihood of developing sepsis. The correlation is close to zero, suggesting no significant relationship.\n",
    "\n",
    "###### In summary, the analysis of the correlations suggests that attributes such as PL, M11 (BMI), and age may have a moderate positive correlation with the likelihood of developing sepsis. However, the other variables have either weak or very weak correlations, indicating limited or no meaningful relationship with sepsis development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataset is stored in a DataFrame called 'df'\n",
    "age_ranges = pd.cut(df['Age'], bins=[20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "grouped_data = df.groupby(age_ranges)\n",
    "\n",
    "# Calculate the count of 'Sepssis' for each age range\n",
    "count_sepsis_by_age = grouped_data['Sepssis'].count()\n",
    "\n",
    "# Plotting the count of 'Sepssis' for each age range\n",
    "ax = count_sepsis_by_age.plot(kind='bar', xlabel='Age Range', ylabel='Sepssis Count', title='Sepssis Count by Age Range')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add data labels\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height()}\", (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1716888",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sepsis_by_age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df5256d",
   "metadata": {},
   "source": [
    "The disparity observed in the counts and mean values of sepsis cases by age range suggests that while the age range (20, 30] has the highest count of sepsis cases (323 occurrences), the mean value of sepsis cases within this age range is relatively lower (0.23) compared to other age ranges.\n",
    "\n",
    "This discrepancy can be attributed to the difference in the population size of each age range. The age range (20, 30] has a larger population size, which results in a higher count of sepsis cases. However, when calculating the mean value, which represents the proportion of sepsis cases within each age range, the percentage of sepsis cases within the (20, 30] age range is relatively lower compared to other age ranges.\n",
    "\n",
    "In other words, while the count of sepsis cases in the (20, 30] age range is high, the proportion of sepsis cases within that age range is relatively lower compared to other age ranges such as (40, 50], (50, 60], and (30, 40]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfa450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the count of 'Sepssis' for each age range\n",
    "mean_sepsis_by_age = grouped_data['Sepssis_Encoded'].mean()\n",
    "# Plotting the count of 'Sepssis' for each age range\n",
    "ax = mean_sepsis_by_age.plot(kind='bar', xlabel='Age Range', ylabel='Mean Sepssis', title='Mean Sepssis by Age Range')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add data labels\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height()}\", (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282243b6",
   "metadata": {},
   "source": [
    "## Multivariate Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlations = df[numerical_vars + ['Sepssis_Encoded']].corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlations, annot=True, cmap='viridis')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars = ['PRG', 'PL', 'PR']\n",
    "sns.pairplot(data=df, vars=numerical_vars, hue='Sepssis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars = ['SK', 'TS', 'M11']\n",
    "sns.pairplot(data=df, vars=numerical_vars, hue='Sepssis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d3214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_vars = ['BD2', 'Age', 'Insurance']\n",
    "sns.pairplot(data=df, vars=numerical_vars, hue='Sepssis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6048c24",
   "metadata": {},
   "source": [
    "## Hypothesis Testing  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee223acf",
   "metadata": {},
   "source": [
    "### - Hypothesis 1: Higher plasma glucose levels (PRG) are associated with an increased risk of developing sepsis.\n",
    "\n",
    "- Null Hypothesis: There is no association between higher plasma glucose levels (PRG) and the risk of developing sepsis.\n",
    "\n",
    "- Alternate Hypothesis: Higher plasma glucose levels (PRG) are associated with an increased risk of developing sepsis.\n",
    "\n",
    "Justification: Elevated glucose levels have been linked to impaired immune function and increased susceptibility to infections, including sepsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581cb0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = df.groupby('Sepssis')\n",
    "\n",
    "positive_group = grouped_data.get_group('Positive')\n",
    "negative_group = grouped_data.get_group('Negative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PRG distribution between the two groups\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df, x='Sepssis', y='PRG')\n",
    "plt.xlabel('Sepssis')\n",
    "plt.ylabel('Plasma Glucose (PRG)')\n",
    "plt.title('Distribution of PRG by Sepssis')\n",
    "plt.show()\n",
    "\n",
    "# Calculate summary statistics\n",
    "positive_prg = positive_group['PRG']\n",
    "negative_prg = negative_group['PRG']\n",
    "\n",
    "positive_mean = positive_prg.mean()\n",
    "positive_median = positive_prg.median()\n",
    "positive_std = positive_prg.std()\n",
    "\n",
    "negative_mean = negative_prg.mean()\n",
    "negative_median = negative_prg.median()\n",
    "negative_std = negative_prg.std()\n",
    "\n",
    "print('Positive Group:')\n",
    "print('Mean PRG:', positive_mean)\n",
    "print('Median PRG:', positive_median)\n",
    "print('Standard Deviation:', positive_std)\n",
    "print()\n",
    "\n",
    "print('Negative Group:')\n",
    "print('Mean PRG:', negative_mean)\n",
    "print('Median PRG:', negative_median)\n",
    "print('Standard Deviation:', negative_std)\n",
    "print()\n",
    "\n",
    "# Perform statistical test (e.g., t-test)\n",
    "t_statistic, p_value = stats.ttest_ind(positive_prg, negative_prg)\n",
    "print('T-Statistic:', t_statistic)\n",
    "print('P-Value:', p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb4d085",
   "metadata": {},
   "source": [
    "###### Based on the provided:\n",
    "\n",
    "- Mean PRG (Plasma Glucose) in the Positive Group (patients with sepsis) is 4.78, while in the Negative Group (patients without sepsis) it is 3.32. This suggests that, on average, patients with sepsis tend to have higher plasma glucose levels compared to those without sepsis.\n",
    "\n",
    "- The median PRG in the Positive Group is 4.0, whereas in the Negative Group it is 2.0. The median represents the middle value of a dataset, and it is less affected by extreme values. This further supports the observation that the central tendency of plasma glucose levels is higher in the Positive Group.\n",
    "\n",
    "- The standard deviation of PRG in the Positive Group is 3.76, and in the Negative Group, it is 3.02. The standard deviation measures the dispersion of data points around the mean. In this case, both groups have relatively high standard deviations, indicating considerable variability in plasma glucose levels within each group.\n",
    "\n",
    "- The t-statistic is 5.17, which indicates a significant difference between the means of the Positive and Negative Groups. A larger absolute t-statistic suggests a stronger evidence of a difference between the groups.\n",
    "\n",
    "- The p-value is 3.15e-07, which is very small. This indicates strong evidence against the null hypothesis (no difference between the groups) and suggests that the difference in mean plasma glucose levels between the groups is statistically significant. \n",
    "\n",
    "- In other words, there is a significant association between higher plasma glucose levels and the risk of developing sepsis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d3d4d",
   "metadata": {},
   "source": [
    "### -  Hypothesis 2: Abnormal blood work results, such as high values of PL, SK, and BD2, are indicative of a higher likelihood of sepsis.\n",
    "\n",
    "- Null Hypothesis: There is no association between abnormal blood work results, such as high values of PL, SK, and BD2, and the likelihood of sepsis.\n",
    "\n",
    "- Alternate Hypothesis: Abnormal blood work results, such as high values of PL, SK, and BD2, are indicative of a higher likelihood of sepsis.\n",
    "\n",
    "Justification: Abnormal blood work results may indicate an ongoing infection or an inflammatory response, which are key factors in sepsis development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243c0f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Variables to compare\n",
    "variables = ['PL', 'SK', 'BD2']\n",
    "\n",
    "# Compare variable distributions between the two groups\n",
    "for var in variables:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=df, x='Sepssis', y=var)\n",
    "    plt.xlabel('Sepssis')\n",
    "    plt.ylabel(var)\n",
    "    plt.title(f'Distribution of {var} by Sepssis')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    positive_var = positive_group[var]\n",
    "    negative_var = negative_group[var]\n",
    "\n",
    "    positive_mean = positive_var.mean()\n",
    "    positive_median = positive_var.median()\n",
    "    positive_std = positive_var.std()\n",
    "\n",
    "    negative_mean = negative_var.mean()\n",
    "    negative_median = negative_var.median()\n",
    "    negative_std = negative_var.std()\n",
    "\n",
    "    print(f'Positive Group ({var}):')\n",
    "    print('Mean:', positive_mean)\n",
    "    print('Median:', positive_median)\n",
    "    print('Standard Deviation:', positive_std)\n",
    "    print()\n",
    "\n",
    "    print(f'Negative Group ({var}):')\n",
    "    print('Mean:', negative_mean)\n",
    "    print('Median:', negative_median)\n",
    "    print('Standard Deviation:', negative_std)\n",
    "    print()\n",
    "\n",
    "    # Perform statistical test (e.g., t-test)\n",
    "    t_statistic, p_value = stats.ttest_ind(positive_var, negative_var)\n",
    "    print('T-Statistic:', t_statistic)\n",
    "    print('P-Value:', p_value)\n",
    "    print('---------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13135411",
   "metadata": {},
   "source": [
    "The results of the hypothesis testing for abnormal blood work results (PL, SK, BD2) as indicators of sepsis are as follows:\n",
    "\n",
    "###### PL (Attribute 2):\n",
    "\n",
    "- The positive group (sepsis) has a higher mean (140.29) compared to the negative group (109.44), indicating that patients with sepsis tend to have higher PL levels.\n",
    "\n",
    "- The median value for both groups (138.0 for positive and 106.0 for negative) also shows a similar trend.\n",
    "- The standard deviation is higher in the positive group (32.80) compared to the negative group (27.12), suggesting more variability in PL levels among sepsis patients.\n",
    "- The t-statistic (12.30) is significantly different from zero, indicating a significant difference in PL levels between the two groups.\n",
    "\n",
    "###### - The p-value (3.68e-31) is very small, indicating strong evidence to reject the null hypothesis that there is no difference in PL levels between the groups. This suggests that higher PL levels are associated with a higher likelihood of sepsis.\n",
    "\n",
    "###### SK (Attribute 4):\n",
    "\n",
    "- The mean SK level is slightly higher in the positive group (22.22) compared to the negative group (19.68), but the difference is not as pronounced as in PL.\n",
    "- The median values are 27.0 for the positive group and 21.0 for the negative group, showing a similar pattern.\n",
    "- The standard deviation is also slightly higher in the positive group (17.88) compared to the negative group (14.88).\n",
    "- The t-statistic (1.85) is smaller compared to PL, indicating a less significant difference in SK levels between the two groups.\n",
    "\n",
    "###### - The p-value (0.06) is relatively higher than the conventional significance level of 0.05, suggesting weaker evidence to reject the null hypothesis. This means that the difference in SK levels between the groups may not be statistically significant.\n",
    "\n",
    "###### BD2 (Attribute 7):\n",
    "\n",
    "- The positive group has a higher mean BD2 level (0.57) compared to the negative group (0.44), indicating a potential association between higher BD2 levels and sepsis.\n",
    "- The median values for both groups also show a similar trend.\n",
    "- The standard deviation is higher in the positive group (0.38) compared to the negative group (0.30), suggesting more variability in BD2 levels among sepsis patients.\n",
    "- The t-statistic (4.51) is significantly different from zero, indicating a significant difference in BD2 levels between the groups.\n",
    "\n",
    "###### - The p-value (7.77e-06) is very small, providing strong evidence to reject the null hypothesis and suggesting that higher BD2 levels are associated with a higher likelihood of sepsis.\n",
    "\n",
    "###### In summary, the results provide evidence to support the alternate hypothesis. Higher levels of PL and BD2 are indicative of a higher likelihood of sepsis. However, the results for SK show a trend towards higher levels in the positive group but with weaker statistical significance. Therefore, while abnormal blood work results, specifically PL and BD2, can be considered as indicators of sepsis, further investigation may be needed to determine the significance of SK levels in relation to sepsis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6d53b",
   "metadata": {},
   "source": [
    "### - Hypothesis 3:  Older patients are more likely to develop sepsis compared to younger patients.\n",
    "\n",
    "- Null Hypothesis: There is no difference in the likelihood of developing sepsis between older and younger patients.\n",
    "\n",
    "- Alternate Hypothesis: Older patients are more likely to develop sepsis compared to younger patients.\n",
    "\n",
    "Justification: Advanced age is a known risk factor for sepsis, as the immune system weakens with age and may be less able to mount an effective response to infections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d6f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Age distribution between the positive and negative groups\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df, x='Sepssis', y='Age')\n",
    "plt.xlabel('Sepssis')\n",
    "plt.ylabel('Age')\n",
    "plt.title('Distribution of Age by Sepssis')\n",
    "plt.show()\n",
    "\n",
    "# Calculate summary statistics\n",
    "positive_age = positive_group['Age']\n",
    "negative_age = negative_group['Age']\n",
    "\n",
    "positive_mean = positive_age.mean()\n",
    "positive_median = positive_age.median()\n",
    "positive_std = positive_age.std()\n",
    "\n",
    "negative_mean = negative_age.mean()\n",
    "negative_median = negative_age.median()\n",
    "negative_std = negative_age.std()\n",
    "\n",
    "print('Positive Group:')\n",
    "print('Mean Age:', positive_mean)\n",
    "print('Median Age:', positive_median)\n",
    "print('Standard Deviation:', positive_std)\n",
    "print()\n",
    "\n",
    "print('Negative Group:')\n",
    "print('Mean Age:', negative_mean)\n",
    "print('Median Age:', negative_median)\n",
    "print('Standard Deviation:', negative_std)\n",
    "print()\n",
    "\n",
    "# Perform statistical test (e.g., t-test)\n",
    "t_statistic, p_value = stats.ttest_ind(positive_age, negative_age)\n",
    "print('T-Statistic:', t_statistic)\n",
    "print('P-Value:', p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e132f4",
   "metadata": {},
   "source": [
    "###### Based on the results of the analysis:\n",
    "\n",
    "###### Positive Group:\n",
    "\n",
    "Mean Age: 36.70 years\n",
    "Median Age: 35.0 years\n",
    "Standard Deviation: 10.90 years\n",
    "\n",
    "###### Negative Group:\n",
    "\n",
    "Mean Age: 31.48 years\n",
    "Median Age: 27.0 years\n",
    "Standard Deviation: 11.91 years\n",
    "The t-statistic value is 5.25, and the p-value is 2.07e-07 (very close to zero).\n",
    "\n",
    "###### Interpretation:\n",
    "The results indicate a statistically significant difference in age between the positive (sepsis) and negative (non-sepsis) groups. The positive group has a higher mean and median age compared to the negative group. Additionally, the standard deviation in the positive group is slightly lower than the negative group, indicating less variability in age among patients with sepsis.\n",
    "\n",
    "Therefore, based on this analysis, there is evidence to support the hypothesis that older patients are more likely to develop sepsis compared to younger patients. The advanced age of patients may be a risk factor for sepsis, potentially due to the weakening of the immune system with age. Therefore the NULL hypothesis can be rejected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce450958",
   "metadata": {},
   "source": [
    "### -  Hypothesis 4: Patients with higher body mass index (BMI) values (M11) have a lower risk of sepsis.\n",
    "\n",
    "- Null Hypothesis: There is no association between body mass index (BMI) values (M11) and the risk of sepsis.\n",
    "\n",
    "- Alternate Hypothesis: Patients with higher body mass index (BMI) values (M11) have a lower risk of sepsis.\n",
    "\n",
    "Justification: Obesity has been associated with a dampened immune response, potentially leading to a decreased risk of developing sepsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba53df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BMI distribution between the two groups\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df, x='Sepssis', y='M11')\n",
    "plt.xlabel('Sepssis')\n",
    "plt.ylabel('Body Mass Index (BMI)')\n",
    "plt.title('Distribution of BMI by Sepssis')\n",
    "plt.show()\n",
    "\n",
    "# Calculate summary statistics\n",
    "positive_bmi = positive_group['M11']\n",
    "negative_bmi = negative_group['M11']\n",
    "\n",
    "positive_mean = positive_bmi.mean()\n",
    "positive_median = positive_bmi.median()\n",
    "positive_std = positive_bmi.std()\n",
    "\n",
    "negative_mean = negative_bmi.mean()\n",
    "negative_median = negative_bmi.median()\n",
    "negative_std = negative_bmi.std()\n",
    "\n",
    "print('Positive Group:')\n",
    "print('Mean BMI:', positive_mean)\n",
    "print('Median BMI:', positive_median)\n",
    "print('Standard Deviation:', positive_std)\n",
    "print()\n",
    "\n",
    "print('Negative Group:')\n",
    "print('Mean BMI:', negative_mean)\n",
    "print('Median BMI:', negative_median)\n",
    "print('Standard Deviation:', negative_std)\n",
    "print()\n",
    "\n",
    "# Perform statistical test (e.g., t-test)\n",
    "t_statistic, p_value = stats.ttest_ind(positive_bmi, negative_bmi)\n",
    "print('T-Statistic:', t_statistic)\n",
    "print('P-Value:', p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dda4f7",
   "metadata": {},
   "source": [
    "##### The results of the analysis for the hypothesis regarding body mass index (BMI) and the risk of sepsis are as follows:\n",
    "\n",
    "##### Positive Group:\n",
    "\n",
    "Mean BMI: 35.3856\n",
    "Median BMI: 34.3\n",
    "Standard Deviation: 7.1959\n",
    "\n",
    "##### Negative Group:\n",
    "\n",
    "Mean BMI: 30.0765\n",
    "Median BMI: 29.9\n",
    "Standard Deviation: 7.8127\n",
    "T-Statistic: 8.13497\n",
    "P-Value: 2.39725e-15\n",
    "\n",
    "##### Interpretation:\n",
    "The results indicate a statistically significant difference in BMI between the positive sepsis group and the negative sepsis group. The positive sepsis group has a higher mean BMI (35.3856) compared to the negative sepsis group (30.0765). The t-statistic of 8.13497 suggests a substantial difference between the two groups.\n",
    "\n",
    "Furthermore, the very small p-value of 2.39725e-15 suggests strong evidence against the null hypothesis (no difference in BMI between the groups). In other words, there is a significant association between higher BMI values and a lower risk of sepsis. This supports the hypothesis that patients with higher BMI values are less likely to develop sepsis.\n",
    "\n",
    "It's important to note that correlation does not imply causation, and additional factors or confounding variables may be influencing this relationship. Therefore, further research and analysis are recommended to gain a deeper understanding of the underlying mechanisms and potential causal relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b4626",
   "metadata": {},
   "source": [
    "###### further analysis: \n",
    "\n",
    "Stratified Analysis: Divide the dataset into subgroups based on BMI ranges and examine the sepsis incidence within each subgroup. This can help identify if there is a specific BMI range that exhibits a stronger association with sepsis risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0cc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BMI ranges\n",
    "bmi_ranges = [0, 18.5, 24.9, 29.9, 100]\n",
    "bmi_labels = ['Underweight', 'Normal', 'Overweight', 'Obese']\n",
    "\n",
    "# Create a new column to represent BMI ranges\n",
    "df['BMI Range'] = pd.cut(df['M11'], bins=bmi_ranges, labels=bmi_labels, include_lowest=True)\n",
    "\n",
    "# Group the data by BMI range and calculate the sepsis incidence\n",
    "grouped = df.groupby('BMI Range')['Sepssis'].count().reset_index()\n",
    "\n",
    "# Plot the sepsis incidence by BMI range\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=grouped, x='BMI Range', y='Sepssis')\n",
    "plt.xlabel('BMI Range')\n",
    "plt.ylabel('Sepsis Incidence')\n",
    "plt.title('Sepsis Incidence by BMI Range')\n",
    "\n",
    "# Add data labels\n",
    "for p in plt.gca().patches:\n",
    "    count = p.get_height()\n",
    "    plt.gca().annotate(f'{count}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7337351",
   "metadata": {},
   "source": [
    "### - Hypothesis 5: Patients without valid insurance cards are more likely to develop sepsis.\n",
    "\n",
    "- Null Hypothesis: There is no association between the absence of valid insurance cards and the likelihood of developing sepsis.\n",
    "\n",
    "- Alternate Hypothesis: Patients without valid insurance cards are more likely to develop sepsis.\n",
    "\n",
    "Justification: Lack of access to healthcare, as indicated by the absence of valid insurance, may delay or hinder early detection and treatment of infections, potentially increasing the risk of sepsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecaf40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot to visualize the distribution of sepsis cases by insurance status\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='Insurance', hue='Sepssis')\n",
    "plt.xlabel('Insurance')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sepsis Cases by Insurance Status')\n",
    "plt.show()\n",
    "\n",
    "# Perform a chi-square test of independence to assess the association between insurance and sepsis\n",
    "crosstab = pd.crosstab(df['Insurance'], df['Sepssis'])\n",
    "chi2, p_value, _, _ = stats.chi2_contingency(crosstab)\n",
    "\n",
    "print('Chi-Square Test of Independence:')\n",
    "print('Chi-Square:', chi2)\n",
    "print('P-Value:', p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27f32f",
   "metadata": {},
   "source": [
    "##### Based on the results of the chi-square test of independence, the chi-square statistic is 2.071 and the p-value is 0.150.\n",
    "\n",
    "The chi-square statistic measures the strength of association between two categorical variables, in this case, the association between insurance status and sepsis. A higher chi-square value indicates a stronger association.\n",
    "\n",
    "The p-value is the probability of obtaining the observed association (or a more extreme association) between the variables if there was no true association in the population. In this case, the p-value is 0.150, which is greater than the conventional significance level of 0.05.\n",
    "\n",
    "#### Interpreting the results:\n",
    "\n",
    "Since the p-value is greater than 0.05, we do not have sufficient evidence to reject the null hypothesis. The null hypothesis states that there is no association between insurance status and the likelihood of developing sepsis. Therefore, based on the available data, we cannot conclude that patients without valid insurance cards are more likely to develop sepsis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90cb2ee",
   "metadata": {},
   "source": [
    "# Feature Processing & Engineering\n",
    "Here is the section to **clean**, **process** the dataset and **create new features**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7dd8d",
   "metadata": {},
   "source": [
    "### Data Imbalance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768063d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define colors for the plots\n",
    "palette = ['#008080', '#FF6347', '#E50000', '#D2691E']\n",
    "\n",
    "# Calculate the percentage of positive and negative values\n",
    "sepsis_counts = df['Sepssis'].value_counts()\n",
    "pie_values = [sepsis_counts[0] / sepsis_counts.sum() * 100, sepsis_counts[1] / sepsis_counts.sum() * 100]\n",
    "\n",
    "# Create the figure and axes for subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 7))\n",
    "\n",
    "# Plot the pie chart on the first subplot\n",
    "axes[0].pie(pie_values, labels=['Negative', 'Positive'], autopct='%1.2f%%', explode=(0.1, 0),\n",
    "            colors=palette[:2], wedgeprops={'edgecolor': 'black', 'linewidth': 1, 'antialiased': True})\n",
    "axes[0].set_title('Sepsis Negative and Positive %')\n",
    "\n",
    "# Plot the countplot on the second subplot\n",
    "sns.countplot(data=df, x='Sepssis', palette=palette[:2], edgecolor='black', ax=axes[1])\n",
    "axes[1].set_xticklabels(['Positive', 'Negative'])\n",
    "axes[1].set_title('Sepsis and Negative')\n",
    "\n",
    "# Add labels to the countplot bars\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccac014",
   "metadata": {},
   "source": [
    "As can be observed the dataset is imbalanced and therefore particular attention needs to be paid in the selection of ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfe7d5",
   "metadata": {},
   "source": [
    "## Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_rows(data):\n",
    "    duplicate_rows = data.duplicated()\n",
    "    num_duplicates = duplicate_rows.sum()\n",
    "    print(\"Number of duplicate rows:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31694a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicate rows in train data\n",
    "check_duplicate_rows(train)\n",
    "\n",
    "# Check duplicate rows in test data\n",
    "check_duplicate_rows(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a6365",
   "metadata": {},
   "source": [
    "## Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(data):\n",
    "    missing_values = data.isna().sum()\n",
    "    print(\"Missing values:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in train data\n",
    "check_missing_values(train)\n",
    "\n",
    "# Check missing values in test data\n",
    "check_missing_values(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0994a53b",
   "metadata": {},
   "source": [
    "## Features Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6467fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target_variable(data, target_variable):\n",
    "    # Encode the target variable using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_target = label_encoder.fit_transform(data[target_variable])\n",
    "    target_encoded = pd.DataFrame(encoded_target, columns=[target_variable])\n",
    "\n",
    "    # Combine the features and the encoded target variable\n",
    "    data_encoded = pd.concat([data.iloc[:, :-1], target_encoded], axis=1)\n",
    "    data_encoded.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "    return data_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9faf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable in train data\n",
    "train_encoded = encode_target_variable(train, 'Sepssis')\n",
    "\n",
    "# Print the encoded train data\n",
    "print(train_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20243f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755a567",
   "metadata": {},
   "source": [
    "## Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eeda37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size, random_state=42, stratify=None):\n",
    "    # Split the data into train and validation sets\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=stratify)\n",
    "\n",
    "    return X_train, X_eval, y_train, y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe576159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation sets for both X and y\n",
    "X_train, X_eval, y_train, y_eval = split_data(train_encoded.iloc[:, :-1], train_encoded.iloc[:, -1:], test_size=0.2, random_state=42, stratify=train_encoded.iloc[:, -1:])\n",
    "\n",
    "# Print the shapes of the train and validation sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_eval shape:\", X_eval.shape)\n",
    "print(\"y_eval shape:\", y_eval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80adb7e1",
   "metadata": {},
   "source": [
    "## Imputting Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23345d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating imputer variables\n",
    "numerical_imputer = SimpleImputer(strategy = \"mean\")\n",
    "\n",
    "numerical_imputer.fit(X_train)\n",
    "\n",
    "X_train_imputed = numerical_imputer.transform(X_train)\n",
    "X_eval_imputed = numerical_imputer.transform(X_eval)\n",
    "X_test_imputed = numerical_imputer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36497da9",
   "metadata": {},
   "source": [
    "## Features Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a43a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imputed)\n",
    "\n",
    "columns = ['PRG','PL','PR','SK','TS','M11','BD2','Age','Insurance']\n",
    "\n",
    "def scale_data(data, scaler, columns):\n",
    "    scaled_data = scaler.transform(data)\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=columns)\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ab530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "X_train_df = scale_data(X_train_imputed, scaler, columns)\n",
    "X_eval_df = scale_data(X_eval_imputed, scaler, columns)\n",
    "X_test = scale_data(X_test_imputed, scaler, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef38b26",
   "metadata": {},
   "source": [
    "# Machine Learning Modeling \n",
    "Here is the section to **build**, **train**, **evaluate** and **compare** the models to each others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd02cb",
   "metadata": {},
   "source": [
    "### Simple Model #001 - Linear regression\n",
    "\n",
    "Please, keep the following structure to try all the model you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b63819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_model(X_train, y_train, X_eval, y_eval):\n",
    "    # Fit logistic regression model\n",
    "    lr_model = LogisticRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the evaluation set\n",
    "    lr_preds = lr_model.predict(X_eval)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    lr_f1_score = f1_score(y_eval, lr_preds)\n",
    "\n",
    "    # Calculate false positive rate, true positive rate, and thresholds using roc_curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval, lr_preds)\n",
    "\n",
    "    # Calculate AUC score\n",
    "    lr_auc_score = roc_auc_score(y_eval, lr_preds)\n",
    "\n",
    "    return lr_model, lr_preds, lr_f1_score, fpr, tpr, thresholds, lr_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and get the outputs\n",
    "lr_model, lr_preds, lr_f1_score, fpr, tpr, thresholds, lr_auc_score = logistic_regression_model(X_train_df, y_train, X_eval_df, y_eval)\n",
    "\n",
    "print(\"F1 Score:\", lr_f1_score)\n",
    "print(\"AUC Score:\", lr_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc0db4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % lr_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be8804",
   "metadata": {},
   "source": [
    "##### Checking for overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4704ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 scores for training and evaluation sets\n",
    "lr_train_f1_score = calculate_f1_score(lr_model, X_train_df, y_train)\n",
    "lr_eval_f1_score = calculate_f1_score(lr_model, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set:\", lr_train_f1_score)\n",
    "print(\"F1 Score on Evaluation Set:\", lr_eval_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(model, X, y):\n",
    "    predictions = model.predict(X)\n",
    "    f1 = f1_score(y, predictions)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e32947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 scores for training and evaluation sets\n",
    "Lr_train_f1_score = calculate_f1_score(lr_model, X_train_df, y_train)\n",
    "lr_eval_f1_score = calculate_f1_score(lr_model, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set:\", Lr_train_f1_score)\n",
    "print(\"F1 Score on Evaluation Set:\", lr_eval_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_validation(model, X, y, cv=5, scoring='f1'):\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "\n",
    "    # Calculate the average score\n",
    "    avg_score = np.mean(cv_scores)\n",
    "\n",
    "    return cv_scores, avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a14075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your logistic regression model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(lr_model, X_train_df, y_train, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average F1 Score:\", avg_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9c709",
   "metadata": {},
   "source": [
    "### Simple Model #002 - Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b96d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_decision_tree(X_train, y_train, X_eval, y_eval):\n",
    "    # Create and fit the decision tree classifier model\n",
    "    dt_model = DecisionTreeClassifier(random_state=42)\n",
    "    dt_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the evaluation set\n",
    "    dt_pred = dt_model.predict(X_eval)\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    dt_f1_score = f1_score(y_eval, dt_pred)\n",
    "\n",
    "    # Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval, dt_pred)\n",
    "\n",
    "    # Calculate the AUC (Area Under the Curve)\n",
    "    dt_auc_score = roc_auc_score(y_eval, dt_pred)\n",
    "    return dt_model, dt_pred, dt_f1_score, dt_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70907c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your train and evaluation data\n",
    "dt_model, dt_pred, dt_f1_score, dt_auc_score = evaluate_decision_tree(X_train_df, y_train, X_eval_df, y_eval)\n",
    "\n",
    "print(\"F1 Score:\", dt_f1_score)\n",
    "print(\"AUC Score:\", dt_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % dt_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for decision Tree Classifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3599b",
   "metadata": {},
   "source": [
    "##### Checking for overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc667921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 scores for training and evaluation sets\n",
    "dt_train_f1_score = calculate_f1_score(dt_model, X_train_df, y_train)\n",
    "dt_eval_f1_score = calculate_f1_score(dt_model, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set:\", dt_train_f1_score)\n",
    "print(\"F1 Score on Evaluation Set:\", dt_eval_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a5591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your logistic regression model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(dt_model, X_train_df, y_train, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average F1 Score:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5705f3e",
   "metadata": {},
   "source": [
    "### Simple Model #003 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa24e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_model(X_train, y_train, X_eval, y_eval):\n",
    "    # Fit Random Forest model\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the evaluation set\n",
    "    rf_preds = rf_model.predict(X_eval)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    rf_f1_score = f1_score(y_eval, rf_preds)\n",
    "\n",
    "    # Calculate false positive rate, true positive rate, and thresholds using roc_curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval, rf_preds)\n",
    "\n",
    "    # Calculate AUC score\n",
    "    rf_auc_score = roc_auc_score(y_eval, rf_preds)\n",
    "\n",
    "    return rf_model, rf_preds, rf_f1_score, fpr, tpr, thresholds, rf_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907da842",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model, rf_preds, rf_f1_score, fpr, tpr, thresholds, rf_auc_score = random_forest_model(X_train, y_train, X_eval, y_eval)\n",
    "\n",
    "print(\"F1 Score:\", rf_f1_score)\n",
    "print(\"AUC Score:\", rf_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f567b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % rf_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for Randon Forest Classifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b11303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 scores for training and evaluation sets\n",
    "rf_train_f1_score = calculate_f1_score(rf_model, X_train_df, y_train)\n",
    "rf_eval_f1_score = calculate_f1_score(rf_model, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set based on Random Forest:\", rf_train_f1_score)\n",
    "print(\"F1 Score on Evaluation Set based on Random Forest:\", rf_eval_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5acd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your Random Forest model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(rf_model, X_train_df, y_train, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average F1 Score:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e3aef",
   "metadata": {},
   "source": [
    "### Simple Model #004 - XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98914082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_model(X_train, y_train, X_eval, y_eval):\n",
    "    # Fit XGBoost model\n",
    "    xgb_model = XGBClassifier()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the evaluation set\n",
    "    xgb_preds = xgb_model.predict(X_eval)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    xgb_f1_score = f1_score(y_eval, xgb_preds)\n",
    "\n",
    "    # Calculate false positive rate, true positive rate, and thresholds using roc_curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval, xgb_preds)\n",
    "\n",
    "    # Calculate AUC score\n",
    "    xgb_auc_score = roc_auc_score(y_eval, xgb_preds)\n",
    "\n",
    "    return xgb_model, xgb_preds, xgb_f1_score, fpr, tpr, thresholds, xgb_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model, xgb_preds, xgb_f1_score, fpr, tpr, thresholds, xgb_auc_score = xgboost_model(X_train_df, y_train, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 score and AUC score\n",
    "print(\"F1 Score on Evaluation Set based on XGBoost:\", xgb_f1_score)\n",
    "print(\"AUC Score on Evaluation Set based on XGBoost:\", xgb_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ca2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % xgb_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for XGBoost Classifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de333aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 scores for training and evaluation sets\n",
    "xgb_train_f1_score = calculate_f1_score(xgb_model, X_train_df, y_train)\n",
    "xgb_eval_f1_score = calculate_f1_score(xgb_model, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set based on XGboost:\", xgb_train_f1_score)\n",
    "print(\"F1 Score on Evaluation Set based on XGboost:\", xgb_eval_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your XGboost model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(xgb_model, X_train_df, y_train, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores based on XGboost:\", cv_scores)\n",
    "print(\"Average F1 Score based on XGboost:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84cc46",
   "metadata": {},
   "source": [
    "### Simple Model #005 - Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_model(X_train, y_train, X_eval, y_eval):\n",
    "    # Fit Naive Bayes model\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the evaluation set\n",
    "    nb_preds = nb_model.predict(X_eval)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    nb_f1_score = f1_score(y_eval, nb_preds)\n",
    "\n",
    "    # Calculate false positive rate, true positive rate, and thresholds using roc_curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval, nb_preds)\n",
    "\n",
    "    # Calculate AUC score\n",
    "    nb_auc_score = roc_auc_score(y_eval, nb_preds)\n",
    "\n",
    "    return nb_model, nb_preds, nb_f1_score, fpr, tpr, thresholds, nb_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd61916",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model, nb_preds, nb_f1_score, fpr, tpr, thresholds, nb_auc_score = naive_bayes_model(X_train_df, y_train, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 score and AUC score\n",
    "print(\"F1 Score on Evaluation Set based on Naive Bayes:\", nb_f1_score)\n",
    "print(\"AUC Score on Evaluation Set based on Naive Bayes:\", nb_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % nb_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for Naive Bayes Model')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb787d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 scores for training and evaluation sets\n",
    "nb_train_f1_score = calculate_f1_score(nb_model, X_train_df, y_train)\n",
    "nb_eval_f1_score = calculate_f1_score(nb_model, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set based on Naive Bayes:\", nb_train_f1_score)\n",
    "print(\"F1 Score on Evaluation Set based on Naive Bayes:\", nb_eval_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69cca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your Naive Bayes model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(nb_model, X_train_df, y_train, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores based on Naive Bayes model:\", cv_scores)\n",
    "print(\"Average F1 Score based on Naive Bayes model:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89566b5b",
   "metadata": {},
   "source": [
    "### Simple Model #006 - Stochastic Grad Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_model_func(X_train, y_train, X_eval, y_eval):\n",
    "    # Fit SGD model\n",
    "    sgd_model = SGDClassifier()\n",
    "    sgd_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the evaluation set\n",
    "    sgd_preds = sgd_model.predict(X_eval)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    sgd_f1_score = f1_score(y_eval, sgd_preds)\n",
    "\n",
    "    # Calculate false positive rate, true positive rate, and thresholds using roc_curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_eval, sgd_preds)\n",
    "\n",
    "    # Calculate AUC score\n",
    "    sgd_auc_score = roc_auc_score(y_eval, sgd_preds)\n",
    "\n",
    "    return sgd_model, sgd_preds, sgd_f1_score, fpr, tpr, thresholds, sgd_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65951f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model, sgd_preds, sgd_f1_score, fpr, tpr, thresholds, sgd_auc_score = sgd_model_func(X_train_df, y_train, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 score and AUC score\n",
    "print(\"F1 Score on Evaluation Set based on SGD:\", sgd_f1_score)\n",
    "print(\"AUC Score on Evaluation Set based on SGD:\", sgd_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96619172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % sgd_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for SGDClassifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf87df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_train_f1_score = calculate_f1_score(sgd_model, X_train_df, y_train)\n",
    "sgd_eval_f1_score = calculate_f1_score(sgd_model, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set based on SGDClassifier:\", sgd_train_f1_score)\n",
    "print(\"F1 Score on Evaluation Set based on SGDClassifier:\", sgd_eval_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your Naive Bayes model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(sgd_model, X_train_df, y_train, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores based on Naive Bayes model:\", cv_scores)\n",
    "print(\"Average F1 Score based on Naive Bayes model:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59389a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_No_SMOTE= {'model':['Decision Tree','Random Forest','XGBoost','Logistic Regression','Naive Bayes','SGBoost'],\n",
    "         'f1_score':[dt_f1_score,rf_f1_score,xgb_f1_score,lr_f1_score,nb_f1_score,sgd_f1_score],\n",
    "         'AUC_score':[dt_auc_score,rf_auc_score,xgb_auc_score,lr_auc_score,nb_auc_score,sgd_auc_score]}\n",
    "\n",
    "results_No_SMOTE_df= pd.DataFrame(results_No_SMOTE)\n",
    "results_No_SMOTE_df_sorted = results_No_SMOTE_df.sort_values(by= 'AUC_score', ascending = False)\n",
    "results_No_SMOTE_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5744f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_No_SMOTE_df_sorted, x='model', y='AUC_score', palette='viridis')\n",
    "\n",
    "# Add data labels\n",
    "for i, value in enumerate(results_No_SMOTE_df_sorted['AUC_score']):\n",
    "    plt.text(i, value, round(value, 2), ha='center', va='bottom')\n",
    "\n",
    "# Set other plot properties\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.title('Comparison of AUC Scores for Different Models')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4b4c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_feature_importance(model, X_train_df, y_train, num_features):\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        # For logistic regression, use the coefficients as feature importance\n",
    "        importances = np.abs(model.coef_[0])\n",
    "    elif isinstance(model, GaussianNB):\n",
    "        # For Naive Bayes, use feature variances as importance\n",
    "        importances = np.var(X_train_df, axis=0)\n",
    "    else:\n",
    "        # For other models, use RFE for feature importance\n",
    "        rfe = RFE(model, n_features_to_select=num_features)\n",
    "        rfe.fit(X_train_df, y_train)\n",
    "        importances = rfe.support_\n",
    "    \n",
    "    return importances\n",
    "\n",
    "# Assuming you have the trained models available\n",
    "models = {\n",
    "    'Decision Tree': dt_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'XGBoost': xgb_model,\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Naive Bayes': nb_model,\n",
    "    'SGBoost': sgd_model\n",
    "}\n",
    "\n",
    "# Define the number of features you want to select\n",
    "num_features = 5\n",
    "\n",
    "# Iterate over each model and display feature importance\n",
    "for model_name, model in models.items():\n",
    "    importances = get_feature_importance(model, X_train_df, y_train, num_features)\n",
    "    features = df.columns\n",
    "\n",
    "    # Get the indices of the selected features\n",
    "    indices = np.where(importances)[0]\n",
    "\n",
    "    # Plotting feature importances using seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=importances[indices], y=features[indices])\n",
    "    plt.title(f'Feature Importance - {model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add data labels to the bar plot\n",
    "    for i, v in enumerate(importances[indices]):\n",
    "        plt.text(v, i, f'{v:.2f}', color='black', ha='left', va='center')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83434fb2",
   "metadata": {},
   "source": [
    "## Optional: Train Dataset Balancing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65327c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Over-sampling/Under-sampling methods, more details here: https://imbalanced-learn.org/stable/install.html\n",
    "oversample= SMOTE()\n",
    "X_train_resampled,y_train_resampled= oversample.fit_resample(X_train_df, y_train)\n",
    "X_train_resampled.shape,y_train_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1a23b",
   "metadata": {},
   "source": [
    "### Balanced Model #001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a05b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the logistic_regression_model function passing the balanced training data and evaluation data\n",
    "lr_model_b, lr_preds_b, lr_f1_score_b, fpr_b, tpr_b, thresholds_b, lr_auc_score_b = logistic_regression_model(X_train_resampled, y_train_resampled, X_eval_df, y_eval)\n",
    "print(\"F1 Score:\", lr_f1_score_b)\n",
    "print(\"AUC Score:\", lr_auc_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % lr_auc_score_b)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for linear regression Classifier (Balanced Data)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_f1_score_b = calculate_f1_score(lr_model_b, X_train_resampled, y_train_resampled)\n",
    "lr_eval_f1_score_b = calculate_f1_score(lr_model_b, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set for Logistic regression with balanced data:\", lr_train_f1_score_b)\n",
    "print(\"F1 Score on Evaluation Set for Logistic regression with balanced data:\", lr_eval_f1_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bb5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your Logistic Regression model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(lr_model_b, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores for Logistic regression with balanced data:\", cv_scores)\n",
    "print(\"Average F1 Score based for Logistic regression with balanced data:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68359f",
   "metadata": {},
   "source": [
    "### Balanced Model #002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99002e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your train and evaluation data\n",
    "dt_model_b, dt_pred_b, dt_f1_score_b, dt_auc_score_b = evaluate_decision_tree(X_train_resampled, y_train_resampled, X_eval_df, y_eval)\n",
    "\n",
    "print(\"F1 Score:\", dt_f1_score_b)\n",
    "print(\"AUC Score:\", dt_auc_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f69378",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % dt_auc_score_b)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for decision tree Classifier (Balanced Data)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc21662",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train_f1_score_b = calculate_f1_score(dt_model_b, X_train_resampled, y_train_resampled)\n",
    "dt_eval_f1_score_b = calculate_f1_score(dt_model_b, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set for decision tree with balanced data:\", dt_train_f1_score_b)\n",
    "print(\"F1 Score on Evaluation Set for decision tree with balanced data:\", dt_eval_f1_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your Naive Bayes model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(dt_model_b, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores for decision tree with balanced data:\", cv_scores)\n",
    "print(\"Average F1 Score for decision tree with balanced data:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d187e5",
   "metadata": {},
   "source": [
    "### Balanced Model #003 - Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c335c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_b, rf_preds_b, rf_f1_score_b, fpr_b, tpr_b, thresholds_b, rf_auc_score_b = random_forest_model(X_train_resampled, y_train_resampled, X_eval, y_eval)\n",
    "\n",
    "print(\"F1 Score for Random Forest with balanced data:\", rf_f1_score_b)\n",
    "print(\"AUC Score for Random Forest with balanced data:\", rf_auc_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % rf_auc_score_b)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for Random Forest Classifier (Balanced Data)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57895f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_f1_score_b = calculate_f1_score(rf_model_b, X_train_resampled, y_train_resampled)\n",
    "rf_eval_f1_score_b = calculate_f1_score(rf_model_b, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set for Random forest with balanced data:\", rf_train_f1_score_b)\n",
    "print(\"F1 Score on Evaluation Set for Random forest with balanced data:\", rf_eval_f1_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your Naive Bayes model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(rf_model_b, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores for Random forest with balanced data:\", cv_scores)\n",
    "print(\"Average F1 Score for Random forest with balanced data:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efdd5d",
   "metadata": {},
   "source": [
    "### Balanced Model #004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_b, xgb_preds_b, xgb_f1_score_b, fpr_b, tpr_b, thresholds_b, xgb_auc_score_b = xgboost_model(X_train_resampled, y_train_resampled, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 score and AUC score\n",
    "print(\"F1 Score on Evaluation for XGBoost with balanced data:\", xgb_f1_score_b)\n",
    "print(\"AUC Score on Evaluation for XGBoost with balanced data:\", xgb_auc_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % xgb_auc_score_b)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for XGBoost (Balanced Data)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_f1_score_b = calculate_f1_score(xgb_model_b, X_train_resampled, y_train_resampled)\n",
    "xgb_eval_f1_score_b = calculate_f1_score(xgb_model_b, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set for XGBoost with balanced data:\", xgb_train_f1_score_b)\n",
    "print(\"F1 Score on Evaluation Set for XGBoost with balanced data:\", xgb_eval_f1_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1581f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your Naive Bayes model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(xgb_model_b, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores for Random forest with balanced data:\", cv_scores)\n",
    "print(\"Average F1 Score for Random forest with balanced data:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57205f",
   "metadata": {},
   "source": [
    "### Balanced Model #005 - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ff629",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model_b, nb_preds_b, nb_f1_score_b, fpr_b, tpr_b, thresholds_b, nb_auc_score_b = naive_bayes_model(X_train_resampled, y_train_resampled, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 score and AUC score\n",
    "print(\"F1 Score on Evaluation Set based for Naive Bayes with balanced data:\", nb_f1_score_b)\n",
    "print(\"AUC Score on Evaluation Set based for Naive Bayes with balanced data:\", nb_auc_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e17aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % nb_auc_score_b)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for Naive Bayes (Balanced Data)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_f1_score_b = calculate_f1_score(nb_model_b, X_train_resampled, y_train_resampled)\n",
    "nb_eval_f1_score_b = calculate_f1_score(nb_model_b, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set for Naive Bayes with balanced data:\", nb_train_f1_score_b)\n",
    "print(\"F1 Score on Evaluation Set for Naive Bayes with balanced data:\", nb_eval_f1_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your Naive Bayes model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(nb_model_b, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores for Random forest with balanced data:\", cv_scores)\n",
    "print(\"Average F1 Score for Random forest with balanced data:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc1459",
   "metadata": {},
   "source": [
    "### Balanced Model #006 - Stochastic Gradient Decent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc6280",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model_b, sgd_preds_b, sgd_f1_score_b, fpr_b, tpr_b, thresholds_b, sgd_auc_score_b = sgd_model_func(X_train_resampled, y_train_resampled, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 score and AUC score\n",
    "print(\"F1 Score on Evaluation Set for SGD with balanced data:\", sgd_f1_score_b)\n",
    "print(\"AUC Score on Evaluation Set for SGD with balanced data:\", sgd_auc_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % sgd_auc_score_b)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve for SGD (Balanced Data)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff15664",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_train_f1_score_b = calculate_f1_score(sgd_model_b, X_train_resampled, y_train_resampled)\n",
    "sgd_eval_f1_score_b = calculate_f1_score(sgd_model_b, X_eval_df, y_eval)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score on Training Set for SGD with balanced data:\", sgd_train_f1_score_b)\n",
    "print(\"F1 Score on Evaluation Set for SGD with balanced data:\", sgd_eval_f1_score_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your Naive Bayes model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(sgd_model_b, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores for SGD with balanced data:\", cv_scores)\n",
    "print(\"Average F1 Score for SGD with balanced data:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d11e80f",
   "metadata": {},
   "source": [
    "## Models comparison\n",
    "Create a pandas dataframe that will allow you to compare your models.\n",
    "\n",
    "Find a sample frame below :\n",
    "\n",
    "|     | Model_Name     | Metric (metric_name)    | Details  |\n",
    "|:---:|:--------------:|:--------------:|:-----------------:|\n",
    "| 0   |  -             |  -             | -                 |\n",
    "| 1   |  -             |  -             | -                 |\n",
    "\n",
    "\n",
    "You might use the pandas dataframe method `.sort_values()` to sort the dataframe regarding the metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "results= {'model':['Decision Tree','Random Forest','XGBoost','Logistic Regression','Naive Bayes','SGBoost',\n",
    "                  'Decision Tree_SMOTE','Random Forest_SMOTE','XGBoost_SMOTE','Logistic Regression_SMOTE','Naive Bayes_SMOTE', 'SGBoost_SMOTE'],\n",
    "         'f1_score':[dt_f1_score,rf_f1_score,xgb_f1_score,lr_f1_score,nb_f1_score,sgd_f1_score,\n",
    "                    dt_f1_score_b,rf_f1_score_b,xgb_f1_score_b,lr_f1_score_b,nb_f1_score_b,sgd_f1_score_b],\n",
    "         'AUC_score':[dt_auc_score,rf_auc_score,xgb_auc_score,lr_auc_score,nb_auc_score,sgd_auc_score,\n",
    "                     dt_auc_score_b,rf_auc_score_b,xgb_auc_score_b,lr_auc_score_b,nb_auc_score_b, sgd_auc_score_b]}\n",
    "\n",
    "results_df= pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by= 'AUC_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter models with and without SMOTE\n",
    "models_with_smote = ['Decision Tree_SMOTE', 'Random Forest_SMOTE', 'XGBoost_SMOTE', 'Logistic Regression_SMOTE', 'Naive Bayes_SMOTE', 'SGBoost_SMOTE']\n",
    "models_without_smote = ['Decision Tree', 'Random Forest', 'XGBoost', 'Logistic Regression', 'Naive Bayes', 'SGBoost']\n",
    "\n",
    "# Create separate DataFrames for each comparison\n",
    "f1_scores_comparison = results_df[results_df['model'].isin(models_without_smote + models_with_smote)][['model', 'f1_score']]\n",
    "auc_scores_comparison = results_df[results_df['model'].isin(models_without_smote + models_with_smote)][['model', 'AUC_score']]\n",
    "\n",
    "# Convert model categories to SMOTE and Non-SMOTE\n",
    "f1_scores_comparison['model_category'] = f1_scores_comparison['model'].apply(lambda x: 'SMOTE' if 'SMOTE' in x else 'Non-SMOTE')\n",
    "auc_scores_comparison['model_category'] = auc_scores_comparison['model'].apply(lambda x: 'SMOTE' if 'SMOTE' in x else 'Non-SMOTE')\n",
    "\n",
    "# Plotting using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='f1_score', y='model', data=f1_scores_comparison, hue='model_category')\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Model')\n",
    "plt.title('F1 Score Comparison')\n",
    "\n",
    "# Add data labels to the bar plot\n",
    "for i, (value, name) in enumerate(zip(f1_scores_comparison['f1_score'], f1_scores_comparison['model'])):\n",
    "    plt.text(value, i, f'{value:.2f}', ha='left', va='center')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='AUC_score', y='model', data=auc_scores_comparison, hue='model_category')\n",
    "plt.xlabel('AUC Score')\n",
    "plt.ylabel('Model')\n",
    "plt.title('AUC Score Comparison')\n",
    "\n",
    "# Add data labels to the bar plot\n",
    "for i, (value, name) in enumerate(zip(auc_scores_comparison['AUC_score'], auc_scores_comparison['model'])):\n",
    "    plt.text(value, i, f'{value:.2f}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc7b30",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning \n",
    "\n",
    "Fine-tune the Top-k models (3 < k < 5) using a ` GridSearchCV`  (that is in sklearn.model_selection\n",
    ") to find the best hyperparameters and achieve the maximum performance of each of the Top-k models, then compare them again to select the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2bcaf6",
   "metadata": {},
   "source": [
    "#### MODEL 1 - Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368dab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lr_grid_search(X, y):\n",
    "    # Define the hyperparameters grid\n",
    "    param_grid = {\n",
    "        'C': [10, 30, 50, 70, 80, 100],  # Inverse of regularization strength\n",
    "        'penalty': ['l1', 'l2'],  # Regularization penalty\n",
    "        'solver': ['liblinear', 'saga'],  # Solver algorithm for optimization\n",
    "        'max_iter': [10, 20, 30, 40, 50]  # Maximum number of iterations\n",
    "    }\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'F1': 'f1',\n",
    "        'ROC AUC': 'roc_auc'\n",
    "    }\n",
    "\n",
    "    # Perform grid search to find the best hyperparameters\n",
    "    lr_model = LogisticRegression()\n",
    "    lr_grid_search = GridSearchCV(lr_model, param_grid, scoring=scoring, cv=5, refit='F1')\n",
    "    lr_grid_search.fit(X, y)\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    results = {\n",
    "        'best_params': lr_grid_search.best_params_,\n",
    "        'best_estimator': lr_grid_search.best_estimator_,\n",
    "        'best_f1_score': lr_grid_search.best_score_,\n",
    "        'best_roc_auc_score': roc_auc_score(y, lr_grid_search.predict_proba(X)[:, 1])\n",
    "    }\n",
    "\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and store the results\n",
    "lr_results = perform_lr_grid_search(X_train_df, y_train)\n",
    "\n",
    "# Access the desired parameters from the results dictionary\n",
    "lr_best_params_h = lr_results['best_params']\n",
    "lr_best_estimator_h = lr_results['best_estimator']\n",
    "lr_best_f1_score_h = lr_results['best_f1_score']\n",
    "lr_best_roc_auc_score_h = lr_results['best_roc_auc_score']\n",
    "\n",
    "# Print the desired parameters\n",
    "print(\"Best hyperparameters: \", lr_best_params_h)\n",
    "print('Best estimator: ', lr_best_estimator_h)\n",
    "print(\"Best F1 score: \", lr_best_f1_score_h)\n",
    "print(\"Best ROC AUC score: \", lr_best_roc_auc_score_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eccfdfd",
   "metadata": {},
   "source": [
    "##### Checking for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your logistic regression model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(lr_best_estimator_h, X_train_df, y_train, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average F1 Score:\", avg_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32696e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the grid search\n",
    "lr_results_h_b = perform_lr_grid_search(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Access the desired parameters from the results dictionary\n",
    "lr_best_params_h_b = lr_results_h_b['best_params']\n",
    "lr_best_estimator_h_b = lr_results_h_b['best_estimator']\n",
    "lr_best_f1_score_h_b = lr_results_h_b['best_f1_score']\n",
    "lr_best_roc_auc_score_h_b = lr_results_h_b['best_roc_auc_score']\n",
    "\n",
    "# Print the desired parameters\n",
    "print(\"Best hyperparameters: \", lr_best_params_h_b)\n",
    "print('Best estimator: ', lr_best_estimator_h_b)\n",
    "print(\"Best F1 score: \", lr_best_f1_score_h_b)\n",
    "print(\"Best ROC AUC score: \", lr_best_roc_auc_score_h_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20146bcb",
   "metadata": {},
   "source": [
    "##### Checking for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your logistic regression model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(lr_best_estimator_h_b, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average F1 Score:\", avg_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd5e240",
   "metadata": {},
   "source": [
    "#### MODEL 2 - Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dt_grid_search(X, y):\n",
    "    # Define the hyperparameters grid\n",
    "    param_grid = {\n",
    "        'max_depth': [None, 5, 10, 15],  # Maximum depth of the tree\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 5],  # Minimum number of samples required to be at a leaf node\n",
    "        'max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "    }\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'F1': 'f1',\n",
    "        'ROC AUC': 'roc_auc'\n",
    "    }\n",
    "\n",
    "    # Perform grid search to find the best hyperparameters\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    dt_grid_search = GridSearchCV(dt_model, param_grid, scoring=scoring, cv=5, refit='F1')\n",
    "    dt_grid_search.fit(X, y)\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    results = {\n",
    "        'best_params': dt_grid_search.best_params_,\n",
    "        'best_estimator': dt_grid_search.best_estimator_,\n",
    "        'best_f1_score': dt_grid_search.best_score_,\n",
    "        'best_roc_auc_score': roc_auc_score(y, dt_grid_search.predict_proba(X)[:, 1])\n",
    "    }\n",
    "\n",
    "    # Return the results\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and store the results\n",
    "dt_results_h = perform_dt_grid_search(X_train_df, y_train)\n",
    "\n",
    "# Access the desired parameters from the results dictionary\n",
    "dt_best_params_h = dt_results_h['best_params']\n",
    "dt_best_estimator_h = dt_results_h['best_estimator']\n",
    "dt_best_f1_score_h = dt_results_h['best_f1_score']\n",
    "dt_best_roc_auc_score_h = dt_results_h['best_roc_auc_score']\n",
    "\n",
    "# Print the desired parameters\n",
    "print(\"Best hyperparameters: \", dt_best_params_h)\n",
    "print('Best estimator: ', dt_best_estimator_h)\n",
    "print(\"Best F1 score: \", dt_best_f1_score_h)\n",
    "print(\"Best ROC AUC score: \", dt_best_roc_auc_score_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45b3f9",
   "metadata": {},
   "source": [
    "##### Checking for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your logistic regression model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(dt_best_estimator_h, X_train_df, y_train, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average F1 Score:\", avg_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and store the results\n",
    "dt_results_h_b = perform_dt_grid_search(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Access the desired parameters from the results dictionary\n",
    "dt_best_params_h_b = dt_results_h_b['best_params']\n",
    "dt_best_estimator_h_b = dt_results_h_b['best_estimator']\n",
    "dt_best_f1_score_h_b = dt_results_h_b['best_f1_score']\n",
    "dt_best_roc_auc_score_h_b = dt_results_h_b['best_roc_auc_score']\n",
    "\n",
    "# Print the desired parameters\n",
    "print(\"Best hyperparameters: \", dt_best_params_h_b)\n",
    "print('Best estimator: ', dt_best_estimator_h_b)\n",
    "print(\"Best F1 score: \", dt_best_f1_score_h_b)\n",
    "print(\"Best ROC AUC score: \", dt_best_roc_auc_score_h_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87640e",
   "metadata": {},
   "source": [
    "##### Checking for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96865b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your logistic regression model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(dt_best_estimator_h_b, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average F1 Score:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a865b71",
   "metadata": {},
   "source": [
    "##### MODEL 3 - XGboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5507ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_xgb_grid_search(X, y):\n",
    "    # Define the hyperparameters grid\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],  # Maximum depth of the tree\n",
    "        'learning_rate': [0.1, 0.01, 0.001],  # Learning rate\n",
    "        'n_estimators': [100, 200, 300],  # Number of trees (estimators)\n",
    "        'subsample': [0.8, 1.0],  # Subsample ratio of the training instances\n",
    "        'colsample_bytree': [0.8, 1.0]  # Subsample ratio of columns when constructing each tree\n",
    "    }\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'F1': 'f1',\n",
    "        'ROC AUC': 'roc_auc'\n",
    "    }\n",
    "\n",
    "    # Perform grid search to find the best hyperparameters\n",
    "    xgb_model = XGBClassifier()\n",
    "    xgb_grid_search = GridSearchCV(xgb_model, param_grid, scoring=scoring, cv=5, refit='F1')\n",
    "    xgb_grid_search.fit(X, y)\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    results = {\n",
    "        'best_params': xgb_grid_search.best_params_,\n",
    "        'best_estimator': xgb_grid_search.best_estimator_,\n",
    "        'best_f1_score': xgb_grid_search.best_score_,\n",
    "        'best_roc_auc_score': roc_auc_score(y, xgb_grid_search.predict_proba(X)[:, 1])\n",
    "    }\n",
    "\n",
    "    # Return the results\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8440f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and store the results\n",
    "xgb_results_h = perform_xgb_grid_search(X_train_df, y_train)\n",
    "\n",
    "# Access the desired parameters from the results dictionary\n",
    "xgb_best_params_h = xgb_results_h['best_params']\n",
    "xgb_best_estimator_h = xgb_results_h['best_estimator']\n",
    "xgb_best_f1_score_h = xgb_results_h['best_f1_score']\n",
    "xgb_best_roc_auc_score_h = xgb_results_h['best_roc_auc_score']\n",
    "\n",
    "# Print the desired parameters\n",
    "print(\"Best hyperparameters: \", xgb_best_params_h)\n",
    "print('Best estimator: ', xgb_best_estimator_h)\n",
    "print(\"Best F1 score: \", xgb_best_f1_score_h)\n",
    "print(\"Best ROC AUC score: \", xgb_best_roc_auc_score_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e42b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your logistic regression model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(xgb_best_estimator_h, X_train, y_train, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average F1 Score:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f877b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and store the results\n",
    "xgb_results_h_b = perform_xgb_grid_search(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Access the desired parameters from the results dictionary\n",
    "xgb_best_params_h_b = xgb_results_h_b['best_params']\n",
    "xgb_best_estimator_h_b = xgb_results_h_b['best_estimator']\n",
    "xgb_best_f1_score_h_b = xgb_results_h_b['best_f1_score']\n",
    "xgb_best_roc_auc_score_h_b = xgb_results_h_b['best_roc_auc_score']\n",
    "\n",
    "# Print the desired parameters\n",
    "print(\"Best hyperparameters: \", xgb_best_params_h_b)\n",
    "print('Best estimator: ', xgb_best_estimator_h_b)\n",
    "print(\"Best F1 score: \", xgb_best_f1_score_h_b)\n",
    "print(\"Best ROC AUC score: \", xgb_best_roc_auc_score_h_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b53ae",
   "metadata": {},
   "source": [
    "##### Checking for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your logistic regression model and train data\n",
    "cv_scores, avg_f1_score = perform_cross_validation(xgb_best_estimator_h_b, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n",
    "# Print the cross-validation scores and average F1 score\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average F1 Score:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559cc83",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning results comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_hyper= {'model':['Logistic_Regression','Logistic_Regression_SMOTE','Decision_Tree','Decision_Tree_SMOTE', 'XGBoost', 'XGBoost_SMOTE'],\n",
    "         'f1_score':[lr_best_f1_score_h,lr_best_f1_score_h_b,dt_best_f1_score_h,dt_best_f1_score_h_b,\n",
    "                    xgb_best_f1_score_h,xgb_best_f1_score_h_b],\n",
    "         'AUC_score':[lr_best_roc_auc_score_h, lr_best_roc_auc_score_h_b,dt_best_roc_auc_score_h, dt_best_roc_auc_score_h_b,\n",
    "                     xgb_best_roc_auc_score_h, xgb_best_roc_auc_score_h_b,]}\n",
    "\n",
    "results_hyper_df= pd.DataFrame(results_hyper)\n",
    "\n",
    "results_hyper_df.sort_values(by= 'AUC_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_final = results_hyper_df.loc[results_hyper_df['AUC_score'].idxmax(), 'model']\n",
    "best_model_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f915c",
   "metadata": {},
   "source": [
    "### Confusion Matrix for Best model -- Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20353b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_eval, lr_preds)\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fcc6c0",
   "metadata": {},
   "source": [
    "### Predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57052571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "lr_test_preds = lr_model.predict(X_test)\n",
    "\n",
    "# Print the predictions\n",
    "print(lr_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf522c",
   "metadata": {},
   "source": [
    "#### MODEL EXPLAINABILITY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bfe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Shap explainer object\n",
    "explainer = shap.Explainer(lr_model, X_train)\n",
    "\n",
    "# Calculate Shapely values\n",
    "shap_values = explainer(X_eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Explanation object from Shapely values\n",
    "shap_explanation = shap.Explanation(values=shap_values, base_values=explainer.expected_value, data=X_eval_df)\n",
    "\n",
    "# Force plot for a specific instance\n",
    "shap.force_plot(shap_explanation.base_values, shap_explanation.values[0], X_eval_df.iloc[0])\n",
    "\n",
    "# Other plots using the Explanation object\n",
    "shap.summary_plot(shap_explanation, X_eval_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e587658",
   "metadata": {},
   "source": [
    "The summary plot displays a horizontal bar chart that represents the mean magnitude of the Shapley values for each feature in the dataset. The bars are sorted in descending order, with the most impactful features on the top. The length of each bar represents the average absolute impact of the corresponding feature on the model's output. This shows that attribute PL has the most impact and Insurance has the least impact \n",
    "\n",
    "The color of each bar indicates the direction of the feature's impact. Positive values are shown in red, indicating that higher feature values contribute to higher predictions, while negative values are shown in blue, indicating the opposite effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336496d",
   "metadata": {},
   "source": [
    "# Export key components\n",
    "Here is the section to **export** the important ML objects that will be use to develop an app: *Encoder, Scaler, ColumnTransformer, Model, Pipeline, etc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Specify the relative path to the destination directory\n",
    "destination = os.path.join(cwd, \"Assets\")\n",
    "\n",
    "# Create the \"export\" directory if it doesn't exist\n",
    "os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "# Export the numerical imputer\n",
    "imputer_filepath = os.path.join(destination, \"numerical_imputer.joblib\")\n",
    "dump(numerical_imputer, imputer_filepath)\n",
    "\n",
    "# Export the scaler\n",
    "scaler_filepath = os.path.join(destination, \"scaler.joblib\")\n",
    "dump(scaler, scaler_filepath)\n",
    "\n",
    "# Export the logistic regression model\n",
    "model_filepath = os.path.join(destination, \"lr_model.joblib\")\n",
    "dump(lr_model, model_filepath)\n",
    "\n",
    "# Print the paths to the exported components\n",
    "print(f\"Numerical Imputer exported to: {imputer_filepath}\")\n",
    "print(f\"Scaler exported to: {scaler_filepath}\")\n",
    "print(f\"Logistic Regression Model exported to: {model_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37908d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list --format=freeze >Assets/requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
