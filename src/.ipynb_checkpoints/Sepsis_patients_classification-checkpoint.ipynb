{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW3v1-2n3CzK"
   },
   "source": [
    "# Sepsis Patient Classification Analysis. \n",
    "\n",
    "# Intro\n",
    "\n",
    "## General\n",
    "\n",
    "In the realm of healthcare, understanding the complex dynamics behind the occurrence of life-threatening conditions is of paramount importance. Sepsis, a potentially fatal condition resulting from the body's extreme response to an infection, remains a major challenge for healthcare providers worldwide. Unraveling the secrets of sepsis occurrence can lead to improved early detection, timely interventions, and ultimately, better patient outcomes.\n",
    "\n",
    "This project aims to delve into the vast pool of patient data, harnessing the power of data analysis and machine learning, to explore patterns and predictors associated with sepsis occurrence. By leveraging advanced computational techniques and drawing insights from comprehensive patient records, this research endeavor seeks to uncover hidden correlations, risk factors, and potential early warning signs that can facilitate earlier diagnosis and intervention.\n",
    "\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "- Hypothesis: Higher plasma glucose levels (PRG) are associated with an increased risk of developing sepsis.\n",
    "Justification: Elevated glucose levels have been linked to impaired immune function and increased susceptibility to infections, including sepsis.\n",
    "\n",
    "- Hypothesis: Abnormal blood work results, such as high values of PL, SK, and BD2, are indicative of a higher likelihood of sepsis.\n",
    "Justification: Abnormal blood work results may indicate an ongoing infection or an inflammatory response, which are key factors in sepsis development.\n",
    "\n",
    "- Hypothesis: Older patients are more likely to develop sepsis compared to younger patients.\n",
    "Justification: Advanced age is a known risk factor for sepsis, as the immune system weakens with age and may be less able to mount an effective response to infections.\n",
    "\n",
    "- Hypothesis: Patients with higher body mass index (BMI) values (M11) have a lower risk of sepsis.\n",
    "Justification: Obesity has been associated with a dampened immune response, potentially leading to a decreased risk of developing sepsis.\n",
    "\n",
    "- Hypothesis: Patients without valid insurance cards are more likely to develop sepsis.\n",
    "Justification: Lack of access to healthcare, as indicated by the absence of valid insurance, may delay or hinder early detection and treatment of infections, potentially increasing the risk of sepsis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4VFUnkuexCE"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdFpRxPje1gw"
   },
   "source": [
    "## Installation\n",
    "Here is the section to install all the packages/libraries that will be needed to tackle the challlenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W-d-roFQe6yi"
   },
   "outputs": [],
   "source": [
    "# !pip install -q <lib_001> <lib_002> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HmLxlQre7HW"
   },
   "source": [
    "## Importation\n",
    "Here is the section to import all the packages/libraries that will be used through this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import scipy.stats as stats\n",
    "\n",
    "#Data Splitting\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Evaluation metrics\n",
    "from sklearn.metrics import f1_score,roc_curve, auc,roc_auc_score\n",
    "#saving model                            \n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MP62JaiKfCnS"
   },
   "outputs": [],
   "source": [
    "# Data handling\n",
    "#import pandas as pd\n",
    "\n",
    "# Vizualisation (Matplotlib, Plotly, Seaborn, etc. )\n",
    "...\n",
    "\n",
    "# EDA (pandas-profiling, etc. )\n",
    "...\n",
    "\n",
    "# Feature Processing (Scikit-learn processing, etc. )\n",
    "...\n",
    "\n",
    "# Machine Learning (Scikit-learn Estimators, Catboost, LightGBM, etc. )\n",
    "...\n",
    "\n",
    "# Hyperparameters Fine-tuning (Scikit-learn hp search, cross-validation, etc. )\n",
    "...\n",
    "\n",
    "# Other packages\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfOADQf0e9i1"
   },
   "source": [
    "# Data Loading\n",
    "Here is the section to load the datasets (train, eval, test) and the additional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIQyG5EcfQlU"
   },
   "outputs": [],
   "source": [
    "# Read the dataset into a pandas DataFrame\n",
    "train = pd.read_csv(r\"C:\\Users\\Nathaniel Havim\\Desktop\\Fin_LP6\\Data\\Paitients_Files_Train.csv\")\n",
    "test =pd.read_csv(r\"C:\\Users\\Nathaniel Havim\\Desktop\\Fin_LP6\\Data\\Paitients_Files_Test.csv\")\n",
    "\n",
    "df = train.copy()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ID: Unique number to represent patient ID\n",
    "\n",
    "- PRG: Plasma glucose\n",
    "\n",
    "- PL: Blood Work Result-1 (mu U/ml)\n",
    "\n",
    "- PR: Blood Pressure (mm Hg)\n",
    "\n",
    "- SK: Blood Work Result-2 (mm)\n",
    "\n",
    "- TS: Blood Work Result-3 (mu U/ml)\n",
    "\n",
    "- M11: Body mass index (weight in kg/(height in m)^2\n",
    "\n",
    "- BD2: Blood Work Result-4 (mu U/ml)\n",
    "\n",
    "- Age\t: patients age (years)\n",
    "\n",
    "- Insurance: If a patient holds a valid insurance card\n",
    "\n",
    "- Sepssis; Positive: if a patient in ICU will develop a sepsis , and Negative: otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okaZxnc3fRId"
   },
   "source": [
    "# Exploratory Data Analysis: EDA\n",
    "Here is the section to **inspect** the datasets in depth, **present** it, make **hypotheses** and **think** the *cleaning, processing and features creation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataset dimension \n",
    "print(\"Number of rows:\", df.shape[0])\n",
    "print(\"Number of columns:\", df.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Explore the summary statistics of numerical columns:\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select columns to plot\n",
    "cols_to_plot = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2', 'Age']\n",
    "\n",
    "# Plot KDEs(kernel density estimation) for all columns\n",
    "fig, axes = plt.subplots(nrows=len(cols_to_plot), figsize=(8, 40))\n",
    "for i, col in enumerate(cols_to_plot):\n",
    "    sns.kdeplot(data=df, x=col, ax=axes[i], fill=True)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    \n",
    "    # Calculate mean, skewness, and kurtosis\n",
    "    mean_val = df[col].mean()\n",
    "    skewness_val = df[col].skew()\n",
    "    kurtosis_val = df[col].kurtosis()\n",
    "    \n",
    "    # Add mean, skewness, and kurtosis as text annotations\n",
    "    axes[i].text(0.6, 0.9, f'Mean: {mean_val:.2f}', transform=axes[i].transAxes)\n",
    "    axes[i].text(0.6, 0.8, f'Skewness: {skewness_val:.2f}', transform=axes[i].transAxes)\n",
    "    axes[i].text(0.6, 0.7, f'Kurtosis: {kurtosis_val:.2f}', transform=axes[i].transAxes)\n",
    "    \n",
    "    # Add mean line\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', label='Mean')\n",
    "    \n",
    "    # Add red dots to indicate potential outliers\n",
    "    outliers = df[(df[col] > mean_val + 3 * df[col].std()) | (df[col] < mean_val - 3 * df[col].std())]\n",
    "    axes[i].plot(outliers[col], [0] * len(outliers), 'ro', label='Potential Outliers')\n",
    "    \n",
    "    # Add legend\n",
    "    axes[i].legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the KDE plot analysis of the PRG variable, it appears that the distribution is positively skewed, suggesting the presence of some higher values. The distribution is also platykurtic, indicating a flatter peak and lighter tails compared to a normal distribution.\n",
    "\n",
    "- Based on the KDE plot analysis of the PL variable, it appears that the distribution is approximately symmetric, with a mean value of 120.15. The distribution is mesokurtic, suggesting a similar shape to a normal distribution. \n",
    "\n",
    "- The kde plot suggests that the blood pressure distribution is negatively skewed and has a more peaked shape with possible outliers.\n",
    "\n",
    "- The kde plot suggests that the distribution of blood work result 2 is slightly positively skewed and has a flatter shape.This suggests that the distribution has fewer outliers or extreme values.\n",
    "\n",
    "- The kde plot suggests that the distribution of TS (blood work result 3) is positively skewed and has a more peaked shape with heavier tails.This means that the tail of the distribution is extended to the right, indicating a higher frequency of lower values compared to higher values.This suggests that the distribution has more outliers or extreme values.\n",
    "\n",
    " - The kde plot suggests that the distribution of body mass index is slightly negatively skewed and has a more peaked shape with heavier tails.This means that the tail of the distribution is extended to the left, indicating a higher frequency of higher values compared to lower values. This suggests that the distribution has more outliers or extreme values.\n",
    " \n",
    "- The kde plot indicates a positively skewed distribution for the blood work result with a more peaked shape and heavier tails.This indicates a higher frequency of extreme values or outliers.This means that the tail of the distribution is extended to the right, suggesting a higher frequency of lower values compared to higher values.\n",
    "\n",
    "- The kde plot indicates a positively skewed distribution of age, with a higher frequency of younger individuals. This indicates a more uniform spread of values without significant outliers or extreme values.This suggests that the tail of the distribution is extended to the right, indicating a higher frequency of younger individuals compared to older individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Count plot for \"Insurance\"\n",
    "sns.countplot(data=df, x='Insurance')\n",
    "\n",
    "# Set labels\n",
    "plt.xlabel('Insurance')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Set title\n",
    "plt.title('Distribution of Insurance')\n",
    "\n",
    "# Calculate percentage distribution\n",
    "total = len(df['Insurance'])\n",
    "percentages = df['Insurance'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Add data labels and percentage annotations\n",
    "for p, percentage in zip(plt.gca().patches, percentages):\n",
    "    count = p.get_height()\n",
    "    percentage_label = f'{percentage:.1f}%'\n",
    "    plt.gca().annotate(f'{count}\\n{percentage_label}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot for \"Sepssis\"\n",
    "sns.countplot(data=df, x='Sepssis')\n",
    "\n",
    "# Set labels\n",
    "plt.xlabel('Sepssis')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Set title\n",
    "plt.title('Distribution of Sepssis')\n",
    "\n",
    "# Calculate percentage distribution\n",
    "total = len(df['Sepssis'])\n",
    "percentages = df['Sepssis'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Add data labels and percentage annotations\n",
    "for p, percentage in zip(plt.gca().patches, percentages):\n",
    "    count = p.get_height()\n",
    "    percentage_label = f'{percentage:.1f}%'\n",
    "    plt.gca().annotate(f'{count}\\n{percentage_label}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Variables - Violin plots with statistics\n",
    "numerical_vars = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2', 'Age']\n",
    "for var in numerical_vars:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(data=df, x='Sepssis', y=var)\n",
    "    plt.xlabel('Sepssis')\n",
    "    plt.ylabel(var)\n",
    "    plt.title(f'{var} Distribution by Sepssis')\n",
    "    \n",
    "    # Calculate statistics\n",
    "    positive_vals = df[df['Sepssis'] == 'Positive'][var]\n",
    "    negative_vals = df[df['Sepssis'] == 'Negative'][var]\n",
    "    stat_dict = {\n",
    "        'Positive': {\n",
    "            'Mean': np.mean(positive_vals),\n",
    "            'Median': np.median(positive_vals),\n",
    "            '25th Percentile': np.percentile(positive_vals, 25),\n",
    "            '75th Percentile': np.percentile(positive_vals, 75)\n",
    "        },\n",
    "        'Negative': {\n",
    "            'Mean': np.mean(negative_vals),\n",
    "            'Median': np.median(negative_vals),\n",
    "            '25th Percentile': np.percentile(negative_vals, 25),\n",
    "            '75th Percentile': np.percentile(negative_vals, 75)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add statistics as text annotations\n",
    "    plt.text(0.30, 0.8, f\"Positive:\\nMean: {stat_dict['Positive']['Mean']:.2f}\\nMedian: {stat_dict['Positive']['Median']:.2f}\\n25th Percentile: {stat_dict['Positive']['25th Percentile']:.2f}\\n75th Percentile: {stat_dict['Positive']['75th Percentile']:.2f}\", transform=plt.gca().transAxes)\n",
    "    plt.text(0.70, 0.8, f\"Negative:\\nMean: {stat_dict['Negative']['Mean']:.2f}\\nMedian: {stat_dict['Negative']['Median']:.2f}\\n25th Percentile: {stat_dict['Negative']['25th Percentile']:.2f}\\n75th Percentile: {stat_dict['Negative']['75th Percentile']:.2f}\", transform=plt.gca().transAxes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Categorical Variables - Bar plots\n",
    "categorical_vars = ['Insurance']\n",
    "for var in categorical_vars:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(data=df, x=var, hue='Sepssis')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'{var} Distribution by Sepssis')\n",
    "\n",
    "    # Calculate percentage distribution\n",
    "    total = len(df['Sepssis'])\n",
    "    percentages = df['Sepssis'].value_counts(normalize=True) * 100\n",
    "\n",
    "    # Add data labels and percentage annotations\n",
    "    for p, percentage in zip(plt.gca().patches, percentages):\n",
    "        count = p.get_height()\n",
    "        percentage_label = f'{percentage:.1f}%'\n",
    "        plt.gca().annotate(f'{count}\\n{percentage_label}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encoding the target variable\n",
    "df['Sepssis_Encoded'] = df['Sepssis'].map({'Negative': 0, 'Positive': 1})\n",
    "\n",
    "# Calculate correlation coefficients\n",
    "numerical_vars = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2', 'Age', 'Insurance']\n",
    "correlations = df[numerical_vars + ['Sepssis_Encoded']].corr()\n",
    "\n",
    "# Print correlation coefficients\n",
    "for var in numerical_vars:\n",
    "    correlation = correlations.loc[var, 'Sepssis_Encoded']\n",
    "    print(f\"Pearson correlation between 'Sepssis_Encoded' and '{var}': {correlation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the correlations between the 'Sepssis_Encoded' variable and each of the other variables:\n",
    "\n",
    "1. 'PRG': The correlation coefficient of 0.21 suggests a weak positive correlation between plasma glucose levels and the likelihood of developing sepsis. However, the correlation is not very strong.\n",
    "\n",
    "2. 'PL': The correlation coefficient of 0.45 indicates a moderate positive correlation between attribute 2 (blood work result-1) and the likelihood of developing sepsis. This suggests that higher values of PL are associated with a higher likelihood of sepsis.\n",
    "\n",
    "3. 'PR': The correlation coefficient of 0.06 indicates a very weak positive correlation between blood pressure and the likelihood of developing sepsis. The correlation is close to zero, suggesting that there is no meaningful relationship between these variables.\n",
    "\n",
    "4. 'SK': The correlation coefficient of 0.08 suggests a very weak positive correlation between attribute 4 (blood work result-2) and the likelihood of developing sepsis. The correlation is close to zero, indicating no significant relationship.\n",
    "\n",
    "5. 'TS': The correlation coefficient of 0.15 indicates a weak positive correlation between attribute 5 (blood work result-3) and the likelihood of developing sepsis. The correlation is not very strong, suggesting a limited relationship.\n",
    "\n",
    "6. 'M11': The correlation coefficient of 0.32 indicates a moderate positive correlation between body mass index (BMI) and the likelihood of developing sepsis. This suggests that higher BMI values are associated with a higher likelihood of sepsis.\n",
    "\n",
    "7. 'BD2': The correlation coefficient of 0.18 suggests a weak positive correlation between attribute 7 (blood work result-4) and the likelihood of developing sepsis. The correlation is not very strong, indicating a limited relationship.\n",
    "\n",
    "8. 'Age': The correlation coefficient of 0.21 suggests a weak positive correlation between age and the likelihood of developing sepsis. This implies that older patients may have a slightly higher likelihood of sepsis.\n",
    "\n",
    "9. 'Insurance': The correlation coefficient of 0.06 indicates a very weak positive correlation between insurance status and the likelihood of developing sepsis. The correlation is close to zero, suggesting no significant relationship.\n",
    "\n",
    "###### In summary, the analysis of the correlations suggests that attributes such as PL, M11 (BMI), and age may have a moderate positive correlation with the likelihood of developing sepsis. However, the other variables have either weak or very weak correlations, indicating limited or no meaningful relationship with sepsis development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlations = df[numerical_vars + ['Sepssis_Encoded']].corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlations, annot=True, cmap='viridis')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars = ['PRG', 'PL', 'PR']\n",
    "sns.pairplot(data=df, vars=numerical_vars, hue='Sepssis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars = ['SK', 'TS', 'M11']\n",
    "sns.pairplot(data=df, vars=numerical_vars, hue='Sepssis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars = ['BD2', 'Age', 'Insurance']\n",
    "sns.pairplot(data=df, vars=numerical_vars, hue='Sepssis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Hypothesis 1: Higher plasma glucose levels (PRG) are associated with an increased risk of developing sepsis.\n",
    "\n",
    "Justification: Elevated glucose levels have been linked to impaired immune function and increased susceptibility to infections, including sepsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into two groups based on sepsis\n",
    "positive_group = df[df['Sepssis'] == 'Positive']\n",
    "negative_group = df[df['Sepssis'] == 'Negative']\n",
    "\n",
    "# Compare PRG distribution between the two groups\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df, x='Sepssis', y='PRG')\n",
    "plt.xlabel('Sepssis')\n",
    "plt.ylabel('Plasma Glucose (PRG)')\n",
    "plt.title('Distribution of PRG by Sepssis')\n",
    "plt.show()\n",
    "\n",
    "# Calculate summary statistics\n",
    "positive_prg = positive_group['PRG']\n",
    "negative_prg = negative_group['PRG']\n",
    "\n",
    "positive_mean = positive_prg.mean()\n",
    "positive_median = positive_prg.median()\n",
    "positive_std = positive_prg.std()\n",
    "\n",
    "negative_mean = negative_prg.mean()\n",
    "negative_median = negative_prg.median()\n",
    "negative_std = negative_prg.std()\n",
    "\n",
    "print('Positive Group:')\n",
    "print('Mean PRG:', positive_mean)\n",
    "print('Median PRG:', positive_median)\n",
    "print('Standard Deviation:', positive_std)\n",
    "print()\n",
    "\n",
    "print('Negative Group:')\n",
    "print('Mean PRG:', negative_mean)\n",
    "print('Median PRG:', negative_median)\n",
    "print('Standard Deviation:', negative_std)\n",
    "print()\n",
    "\n",
    "# Perform statistical test (e.g., t-test)\n",
    "t_statistic, p_value = stats.ttest_ind(positive_prg, negative_prg)\n",
    "print('T-Statistic:', t_statistic)\n",
    "print('P-Value:', p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Based on the provided:\n",
    "\n",
    "- Mean PRG (Plasma Glucose) in the Positive Group (patients with sepsis) is 4.78, while in the Negative Group (patients without sepsis) it is 3.32. This suggests that, on average, patients with sepsis tend to have higher plasma glucose levels compared to those without sepsis.\n",
    "\n",
    "- The median PRG in the Positive Group is 4.0, whereas in the Negative Group it is 2.0. The median represents the middle value of a dataset, and it is less affected by extreme values. This further supports the observation that the central tendency of plasma glucose levels is higher in the Positive Group.\n",
    "\n",
    "- The standard deviation of PRG in the Positive Group is 3.76, and in the Negative Group, it is 3.02. The standard deviation measures the dispersion of data points around the mean. In this case, both groups have relatively high standard deviations, indicating considerable variability in plasma glucose levels within each group.\n",
    "\n",
    "- The t-statistic is 5.17, which indicates a significant difference between the means of the Positive and Negative Groups. A larger absolute t-statistic suggests a stronger evidence of a difference between the groups.\n",
    "\n",
    "- The p-value is 3.15e-07, which is very small. This indicates strong evidence against the null hypothesis (no difference between the groups) and suggests that the difference in mean plasma glucose levels between the groups is statistically significant. In other words, there is a significant association between higher plasma glucose levels and the risk of developing sepsis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -  Hypothesis 2: Abnormal blood work results, such as high values of PL, SK, and BD2, are indicative of a higher likelihood of sepsis.\n",
    "Justification: Abnormal blood work results may indicate an ongoing infection or an inflammatory response, which are key factors in sepsis development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split the data into two groups based on sepsis\n",
    "positive_group = df[df['Sepssis'] == 'Positive']\n",
    "negative_group = df[df['Sepssis'] == 'Negative']\n",
    "\n",
    "# Variables to compare\n",
    "variables = ['PL', 'SK', 'BD2']\n",
    "\n",
    "# Compare variable distributions between the two groups\n",
    "for var in variables:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=df, x='Sepssis', y=var)\n",
    "    plt.xlabel('Sepssis')\n",
    "    plt.ylabel(var)\n",
    "    plt.title(f'Distribution of {var} by Sepssis')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    positive_var = positive_group[var]\n",
    "    negative_var = negative_group[var]\n",
    "\n",
    "    positive_mean = positive_var.mean()\n",
    "    positive_median = positive_var.median()\n",
    "    positive_std = positive_var.std()\n",
    "\n",
    "    negative_mean = negative_var.mean()\n",
    "    negative_median = negative_var.median()\n",
    "    negative_std = negative_var.std()\n",
    "\n",
    "    print(f'Positive Group ({var}):')\n",
    "    print('Mean:', positive_mean)\n",
    "    print('Median:', positive_median)\n",
    "    print('Standard Deviation:', positive_std)\n",
    "    print()\n",
    "\n",
    "    print(f'Negative Group ({var}):')\n",
    "    print('Mean:', negative_mean)\n",
    "    print('Median:', negative_median)\n",
    "    print('Standard Deviation:', negative_std)\n",
    "    print()\n",
    "\n",
    "    # Perform statistical test (e.g., t-test)\n",
    "    t_statistic, p_value = stats.ttest_ind(positive_var, negative_var)\n",
    "    print('T-Statistic:', t_statistic)\n",
    "    print('P-Value:', p_value)\n",
    "    print('---------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the hypothesis testing for abnormal blood work results (PL, SK, BD2) as indicators of sepsis are as follows:\n",
    "\n",
    "###### PL (Attribute 2):\n",
    "\n",
    "- The positive group (sepsis) has a higher mean (140.29) compared to the negative group (109.44), indicating that patients with sepsis tend to have higher PL levels.\n",
    "\n",
    "- The median value for both groups (138.0 for positive and 106.0 for negative) also shows a similar trend.\n",
    "- The standard deviation is higher in the positive group (32.80) compared to the negative group (27.12), suggesting more variability in PL levels among sepsis patients.\n",
    "- The t-statistic (12.30) is significantly different from zero, indicating a significant difference in PL levels between the two groups.\n",
    "- The p-value (3.68e-31) is very small, indicating strong evidence to reject the null hypothesis that there is no difference in PL levels between the groups. This suggests that higher PL levels are associated with a higher likelihood of sepsis.\n",
    "\n",
    "###### SK (Attribute 4):\n",
    "\n",
    "- The mean SK level is slightly higher in the positive group (22.22) compared to the negative group (19.68), but the difference is not as pronounced as in PL.\n",
    "- The median values are 27.0 for the positive group and 21.0 for the negative group, showing a similar pattern.\n",
    "- The standard deviation is also slightly higher in the positive group (17.88) compared to the negative group (14.88).\n",
    "- The t-statistic (1.85) is smaller compared to PL, indicating a less significant difference in SK levels between the two groups.\n",
    "- The p-value (0.06) is relatively higher than the conventional significance level of 0.05, suggesting weaker evidence to reject the null hypothesis. This means that the difference in SK levels between the groups may not be statistically significant.\n",
    "\n",
    "###### BD2 (Attribute 7):\n",
    "\n",
    "- The positive group has a higher mean BD2 level (0.57) compared to the negative group (0.44), indicating a potential association between higher BD2 levels and sepsis.\n",
    "- The median values for both groups also show a similar trend.\n",
    "- The standard deviation is higher in the positive group (0.38) compared to the negative group (0.30), suggesting more variability in BD2 levels among sepsis patients.\n",
    "- The t-statistic (4.51) is significantly different from zero, indicating a significant difference in BD2 levels between the groups.\n",
    "- The p-value (7.77e-06) is very small, providing strong evidence to reject the null hypothesis and suggesting that higher BD2 levels are associated with a higher likelihood of sepsis.\n",
    "\n",
    "In summary, the results suggest that abnormal blood work results, specifically higher levels of PL and BD2, are indicative of a higher likelihood of sepsis. The results for SK show a trend towards higher levels in the positive group but with weaker statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Hypothesis 3: Older patients are more likely to develop sepsis compared to younger patients.\n",
    "Justification: Advanced age is a known risk factor for sepsis, as the immune system weakens with age and may be less able to mount an effective response to infections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Age distribution between the positive and negative groups\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df, x='Sepssis', y='Age')\n",
    "plt.xlabel('Sepssis')\n",
    "plt.ylabel('Age')\n",
    "plt.title('Distribution of Age by Sepssis')\n",
    "plt.show()\n",
    "\n",
    "# Split the data into two groups based on sepsis\n",
    "positive_group = df[df['Sepssis'] == 'Positive']\n",
    "negative_group = df[df['Sepssis'] == 'Negative']\n",
    "\n",
    "# Calculate summary statistics\n",
    "positive_age = positive_group['Age']\n",
    "negative_age = negative_group['Age']\n",
    "\n",
    "positive_mean = positive_age.mean()\n",
    "positive_median = positive_age.median()\n",
    "positive_std = positive_age.std()\n",
    "\n",
    "negative_mean = negative_age.mean()\n",
    "negative_median = negative_age.median()\n",
    "negative_std = negative_age.std()\n",
    "\n",
    "print('Positive Group:')\n",
    "print('Mean Age:', positive_mean)\n",
    "print('Median Age:', positive_median)\n",
    "print('Standard Deviation:', positive_std)\n",
    "print()\n",
    "\n",
    "print('Negative Group:')\n",
    "print('Mean Age:', negative_mean)\n",
    "print('Median Age:', negative_median)\n",
    "print('Standard Deviation:', negative_std)\n",
    "print()\n",
    "\n",
    "# Perform statistical test (e.g., t-test)\n",
    "t_statistic, p_value = stats.ttest_ind(positive_age, negative_age)\n",
    "print('T-Statistic:', t_statistic)\n",
    "print('P-Value:', p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Based on the results of the analysis:\n",
    "\n",
    "###### Positive Group:\n",
    "\n",
    "Mean Age: 36.70 years\n",
    "Median Age: 35.0 years\n",
    "Standard Deviation: 10.90 years\n",
    "\n",
    "###### Negative Group:\n",
    "\n",
    "Mean Age: 31.48 years\n",
    "Median Age: 27.0 years\n",
    "Standard Deviation: 11.91 years\n",
    "The t-statistic value is 5.25, and the p-value is 2.07e-07 (very close to zero).\n",
    "\n",
    "###### Interpretation:\n",
    "The results indicate a statistically significant difference in age between the positive (sepsis) and negative (non-sepsis) groups. The positive group has a higher mean and median age compared to the negative group. Additionally, the standard deviation in the positive group is slightly lower than the negative group, indicating less variability in age among patients with sepsis.\n",
    "\n",
    "Therefore, based on this analysis, there is evidence to support the hypothesis that older patients are more likely to develop sepsis compared to younger patients. The advanced age of patients may be a risk factor for sepsis, potentially due to the weakening of the immune system with age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Hypothesis 4: Patients with higher body mass index (BMI) values (M11) have a lower risk of sepsis.\n",
    "\n",
    "Justification: Obesity has been associated with a dampened immune response, potentially leading to a decreased risk of developing sepsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into two groups based on sepsis\n",
    "positive_group = df[df['Sepssis'] == 'Positive']\n",
    "negative_group = df[df['Sepssis'] == 'Negative']\n",
    "\n",
    "# Compare BMI distribution between the two groups\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df, x='Sepssis', y='M11')\n",
    "plt.xlabel('Sepssis')\n",
    "plt.ylabel('Body Mass Index (BMI)')\n",
    "plt.title('Distribution of BMI by Sepssis')\n",
    "plt.show()\n",
    "\n",
    "# Calculate summary statistics\n",
    "positive_bmi = positive_group['M11']\n",
    "negative_bmi = negative_group['M11']\n",
    "\n",
    "positive_mean = positive_bmi.mean()\n",
    "positive_median = positive_bmi.median()\n",
    "positive_std = positive_bmi.std()\n",
    "\n",
    "negative_mean = negative_bmi.mean()\n",
    "negative_median = negative_bmi.median()\n",
    "negative_std = negative_bmi.std()\n",
    "\n",
    "print('Positive Group:')\n",
    "print('Mean BMI:', positive_mean)\n",
    "print('Median BMI:', positive_median)\n",
    "print('Standard Deviation:', positive_std)\n",
    "print()\n",
    "\n",
    "print('Negative Group:')\n",
    "print('Mean BMI:', negative_mean)\n",
    "print('Median BMI:', negative_median)\n",
    "print('Standard Deviation:', negative_std)\n",
    "print()\n",
    "\n",
    "# Perform statistical test (e.g., t-test)\n",
    "t_statistic, p_value = stats.ttest_ind(positive_bmi, negative_bmi)\n",
    "print('T-Statistic:', t_statistic)\n",
    "print('P-Value:', p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The results of the analysis for the hypothesis regarding body mass index (BMI) and the risk of sepsis are as follows:\n",
    "\n",
    "##### Positive Group:\n",
    "\n",
    "Mean BMI: 35.3856\n",
    "Median BMI: 34.3\n",
    "Standard Deviation: 7.1959\n",
    "\n",
    "##### Negative Group:\n",
    "\n",
    "Mean BMI: 30.0765\n",
    "Median BMI: 29.9\n",
    "Standard Deviation: 7.8127\n",
    "T-Statistic: 8.13497\n",
    "P-Value: 2.39725e-15\n",
    "\n",
    "##### Interpretation:\n",
    "The results indicate a statistically significant difference in BMI between the positive sepsis group and the negative sepsis group. The positive sepsis group has a higher mean BMI (35.3856) compared to the negative sepsis group (30.0765). The t-statistic of 8.13497 suggests a substantial difference between the two groups.\n",
    "\n",
    "Furthermore, the very small p-value of 2.39725e-15 suggests strong evidence against the null hypothesis (no difference in BMI between the groups). In other words, there is a significant association between higher BMI values and a lower risk of sepsis. This supports the hypothesis that patients with higher BMI values are less likely to develop sepsis.\n",
    "\n",
    "It's important to note that correlation does not imply causation, and additional factors or confounding variables may be influencing this relationship. Therefore, further research and analysis are recommended to gain a deeper understanding of the underlying mechanisms and potential causal relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Hypothesis 5: Patients without valid insurance cards are more likely to develop sepsis.\n",
    "\n",
    "Justification: Lack of access to healthcare, as indicated by the absence of valid insurance, may delay or hinder early detection and treatment of infections, potentially increasing the risk of sepsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a countplot to visualize the distribution of sepsis cases by insurance status\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='Insurance', hue='Sepssis')\n",
    "plt.xlabel('Insurance')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sepsis Cases by Insurance Status')\n",
    "plt.show()\n",
    "\n",
    "# Perform a chi-square test of independence to assess the association between insurance and sepsis\n",
    "crosstab = pd.crosstab(df['Insurance'], df['Sepssis'])\n",
    "chi2, p_value, _, _ = stats.chi2_contingency(crosstab)\n",
    "\n",
    "print('Chi-Square Test of Independence:')\n",
    "print('Chi-Square:', chi2)\n",
    "print('P-Value:', p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on the results of the chi-square test of independence, the chi-square statistic is 2.071 and the p-value is 0.150.\n",
    "\n",
    "The chi-square statistic measures the strength of association between two categorical variables, in this case, the association between insurance status and sepsis. A higher chi-square value indicates a stronger association.\n",
    "\n",
    "The p-value is the probability of obtaining the observed association (or a more extreme association) between the variables if there was no true association in the population. In this case, the p-value is 0.150, which is greater than the conventional significance level of 0.05.\n",
    "\n",
    "#### Interpreting the results:\n",
    "\n",
    "Since the p-value is greater than 0.05, we do not have sufficient evidence to reject the null hypothesis. The null hypothesis states that there is no association between insurance status and the likelihood of developing sepsis. Therefore, based on the available data, we cannot conclude that patients without valid insurance cards are more likely to develop sepsis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8WB7j-3fxzL"
   },
   "source": [
    "# Feature Processing & Engineering\n",
    "Here is the section to **clean**, **process** the dataset and **create new features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6Rh5SimmcGe"
   },
   "source": [
    "## Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIBrDy1Pmo2_"
   },
   "outputs": [],
   "source": [
    "# Use pandas.DataFrame.drop_duplicates method\n",
    "\n",
    "# Check for duplicate rows in churn_data\n",
    "duplicate_rows = train.duplicated()\n",
    "print(\"Number of duplicate rows:\", duplicate_rows.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLJuYG_JgBH8"
   },
   "source": [
    "## Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52POL1F8gLEa"
   },
   "outputs": [],
   "source": [
    "# Use sklearn.impute.SimpleImputer\n",
    "\n",
    "# Check for missing values in churn_data\n",
    "missing_values = train.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYkNdwQ-gRod"
   },
   "source": [
    "## Features Encoding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99vphpwPgbUw"
   },
   "outputs": [],
   "source": [
    "# From sklearn.preprocessing use OneHotEncoder to encode the categorical features.\n",
    "# Encoding the target variable\n",
    "label_encoder=LabelEncoder()\n",
    "train_encoded = label_encoder.fit_transform(train['Sepssis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoded = pd.DataFrame(train_encoded,columns=['Sepssis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the features and the encoded target variables\n",
    "train_df = pd.concat([train.iloc[:,:-1], target_encoded], axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxYGw6zug8lI"
   },
   "source": [
    "## Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbsbMbs1hDzo"
   },
   "outputs": [],
   "source": [
    "# Use train_test_split with a random_state, and add stratify for Classification\n",
    "# Split the  data into train and validation sets\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(train_df.iloc[:, :-1], train_df.iloc[:, -1:],\n",
    "                                                    test_size=0.2, random_state=42, stratify=train_df.iloc[:, -1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_eval.shape,y_train.shape,y_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputting Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating imputer variables\n",
    "numerical_imputer = SimpleImputer(strategy = \"mean\")\n",
    "numerical_imputer.fit(X_train)\n",
    "\n",
    "X_train_imputed = numerical_imputer.transform(X_train)\n",
    "X_eval_imputed = numerical_imputer.transform(X_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0TvhRVghEcP"
   },
   "source": [
    "## Features Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pf6WtuHphLBE"
   },
   "outputs": [],
   "source": [
    "# From sklearn.preprocessing use StandardScaler, MinMaxScaler, etc.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imputed)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train_imputed)\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns = ['PRG','PL','PR','SK','TS','M11','BD2','Age','Insurance'])\n",
    "\n",
    "X_eval_scaled = scaler.transform(X_eval_imputed)\n",
    "X_eval_df = pd.DataFrame(X_eval_scaled, columns = ['PRG','PL','PR','SK','TS','M11','BD2','Age','Insurance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WloQgMqqf6bT"
   },
   "source": [
    "# Machine Learning Modeling \n",
    "Here is the section to **build**, **train**, **evaluate** and **compare** the models to each others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTiiqCnG1FA2"
   },
   "source": [
    "## Simple Model #001\n",
    "\n",
    "Please, keep the following structure to try all the model you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit logistic regression model on train data\n",
    "Lrmodel = LogisticRegression()\n",
    "Lrmodel.fit(X_train_df, y_train)\n",
    "\n",
    "#make predictions on validation set\n",
    "Lrpreds = Lrmodel.predict(X_eval_df)\n",
    "\n",
    "lr_f1Score =f1_score(y_eval, Lrpreds)\n",
    "lr_f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, Lrpreds)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "lr_auc_score = roc_auc_score(y_eval, Lrpreds)\n",
    "lr_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % lr_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the AUC score of 0.7197802197802198 suggests that the model's performance is relatively good, but there is still room for improvement. It demonstrates that the model's ability to distinguish between the positive and negative classes is better than random guessing, but there may be some misclassifications or areas where the model's predictions could be further refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_predictions = Lrmodel.predict(X_train_df)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train, train_predictions)\n",
    "\n",
    "\n",
    "\n",
    "eval_predictions = Lrmodel.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two scores, we can see that the F1 score on the training dataset (0.649) is slightly higher than the F1 score on the test dataset (0.627). This suggests that the model may be slightly overfitting the training data, as it performs slightly worse on unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46li5Z2z1ME7"
   },
   "source": [
    "## Simple Model #002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Fjv6pJt1uoV"
   },
   "outputs": [],
   "source": [
    "#fitting decision tree classifier model to the imbalanced train dataset\n",
    "dt_model =DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train_df,y_train)\n",
    "# evaluating the model\n",
    "dt_pred=  dt_model.predict(X_eval_df)\n",
    "dt_f1score =f1_score(y_eval, dt_pred)\n",
    "dt_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, dt_pred)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "dt_auc_score = roc_auc_score(y_eval, dt_pred)\n",
    "dt_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % dt_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_predictions = dt_model.predict(X_train_df)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train, train_predictions)\n",
    "\n",
    "eval_predictions = dt_model.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided f1_score values, it appears that the model is likely overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A54gPujOmqaV"
   },
   "source": [
    "## Simple Model #003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THTw7OjhmqaW"
   },
   "outputs": [],
   "source": [
    "#fitting the model with imbalanced train dataset\n",
    "rf_model= RandomForestClassifier(random_state= 42)\n",
    "rf_model.fit(X_train_df,y_train)\n",
    "# evaluatinng model performance\n",
    "rf_pred= rf_model.predict(X_eval_df)\n",
    "rf_f1Score = f1_score(y_eval, rf_pred)\n",
    "rf_f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, rf_pred)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "rf_auc_score = roc_auc_score(y_eval, rf_pred)\n",
    "rf_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % rf_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_predictions = rf_model.predict(X_train_df)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train, train_predictions)\n",
    "\n",
    "\n",
    "\n",
    "eval_predictions = rf_model.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7CV9W0DmqaW"
   },
   "source": [
    "## Simple Model #004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQMdz0JPmqaW"
   },
   "outputs": [],
   "source": [
    "#fitting model on imbalanced data\n",
    "XGBmodel = XGBClassifier()\n",
    "XGBmodel.fit(X_train_df, y_train)\n",
    "\n",
    "#make predictions on validation set\n",
    "XGBpreds = XGBmodel.predict(X_eval_df)\n",
    "\n",
    "#evaluation\n",
    "xg_f1Score =f1_score(y_eval, XGBpreds)\n",
    "xg_f1Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, XGBpreds)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "xg_auc_score = roc_auc_score(y_eval, XGBpreds)\n",
    "xg_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % xg_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_predictions = XGBmodel.predict(X_train_df)\n",
    "# training accuracy\n",
    "train_score= f1_score(y_train, train_predictions)\n",
    "\n",
    "\n",
    "\n",
    "eval_predictions = XGBmodel.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided f1_score values, it appears that the model is likely overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model #005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "# Train the model on the resampled training data\n",
    "nb_model.fit(X_train_df, y_train)\n",
    "#make predictions on validation set\n",
    "nbpreds = XGBmodel.predict(X_eval_df)\n",
    "\n",
    "nb_f1Score =f1_score(y_eval, nbpreds)\n",
    "nb_f1Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, nbpreds)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "nb_auc_score = roc_auc_score(y_eval, nbpreds)\n",
    "nb_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % nb_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_predictions = nb_model.predict(X_train_df)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train, train_predictions)\n",
    "\n",
    "\n",
    "\n",
    "eval_predictions = nb_model.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided f1_score values, it seems that the model is overfitting the training data. The f1_score on the training data (0.6731) is significantly higher than the f1_score on the test data (0.5747), indicating that the model is not generalizing well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model #006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KNeighborClassifier object with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train th model using the training data\n",
    "knn.fit(X_train_df, y_train)\n",
    "\n",
    "# make predictions on validation set\n",
    "knnpred = knn.predict(X_eval_df)\n",
    "\n",
    "kn_f1Score =f1_score(y_eval, knnpred)\n",
    "kn_f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, knnpred)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "kn_auc_score = roc_auc_score(y_eval, knnpred)\n",
    "kn_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % kn_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_pred = knn.predict(X_train_df)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train, train_pred)\n",
    "\n",
    "\n",
    "# make prediction on unseen data\n",
    "eval_pred = knn.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_pred)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model #007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SVM classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train th model using the training data\n",
    "svc.fit(X_train_df, y_train)\n",
    "\n",
    "# make predictions on validation set\n",
    "svcpred = svc.predict(X_eval_df)\n",
    "\n",
    "svc_f1Score =f1_score(y_eval, svcpred)\n",
    "svc_f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, svcpred)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "svc_auc_score = roc_auc_score(y_eval, svcpred)\n",
    "svc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % svc_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_preds = svc.predict(X_train_df)\n",
    "# training accuracy\n",
    "train_scores = f1_score(y_train, train_preds)\n",
    "\n",
    "\n",
    "# make prediction on unseen data\n",
    "eval_pred = svc.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_pred)\n",
    "print(f\"f1_score on train score {train_scores}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Train Dataset Balancing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Over-sampling/Under-sampling methods, more details here: https://imbalanced-learn.org/stable/install.html\n",
    "oversample= SMOTE()\n",
    "X_train_resampled,y_train_resampled= oversample.fit_resample(X_train_df, y_train)\n",
    "X_train_resampled.shape,y_train_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Model #001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit logistic regression model on rrain data\n",
    "Lrmodel = LogisticRegression()\n",
    "Lrmodel.fit(X_train_resampled,y_train_resampled)\n",
    "\n",
    "#make predictions on validation set\n",
    "Lr_preds = Lrmodel.predict(X_eval_df)\n",
    "\n",
    "lr_f1Score = f1_score(y_eval, Lr_preds)\n",
    "lr_f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, Lr_preds)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "lr_auc_score = roc_auc_score(y_eval, Lr_preds)\n",
    "lr_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % lr_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_predictions = Lrmodel.predict(X_train_resampled)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train_resampled, train_predictions)\n",
    "\n",
    "\n",
    "eval_predictions = Lrmodel.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_predictions)\n",
    "\n",
    "\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided f1_score values, it appears that the model is likely overfitting the data with a train score (0.7495) higher than the test data score (0.6597)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Model #002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting model to the imbalanced train dataset\n",
    "dt_model.fit(X_train_resampled,y_train_resampled)\n",
    "# evaluating the model\n",
    "dtpred=  dt_model.predict(X_eval_df)\n",
    "dt_f1Score = f1_score(y_eval, dtpred)\n",
    "dt_f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, dtpred)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "dt_auc_score = roc_auc_score(y_eval, dtpred)\n",
    "dt_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % dt_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_predictions = dt_model.predict(X_train_resampled)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train_resampled, train_predictions)\n",
    "\n",
    "\n",
    "eval_predictions = dt_model.predict(X_eval_df)\n",
    "eval_score= f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided f1_score values, it appears that the model is likely overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Model #003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model with imbalanced train dataset\n",
    "rf_model.fit(X_train_resampled,y_train_resampled)\n",
    "\n",
    "# evaluatinng model performance\n",
    "rfpred= rf_model.predict(X_eval_df)\n",
    "rf_f1Score = f1_score(y_eval, rfpred)\n",
    "rf_f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, rfpred)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "rf_auc_score = roc_auc_score(y_eval, rfpred)\n",
    "rf_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % rf_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate predictions on training set\n",
    "train_predictions = rf_model.predict(X_train_resampled)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train_resampled, train_predictions)\n",
    "\n",
    "eval_predictions = rf_model.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided f1_score values, it appears that the model is likely overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Model #004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBmodel.fit(X_train_resampled,y_train_resampled)\n",
    "\n",
    "#make predictions on validation set\n",
    "XGB_preds = XGBmodel.predict(X_eval_df)\n",
    "\n",
    "xg_f1Score = f1_score(y_eval, XGB_preds)\n",
    "xg_f1Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, XGB_preds)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "xg_auc_score = roc_auc_score(y_eval, XGB_preds)\n",
    "xg_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % xg_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate predictions on training set\n",
    "train_predictions = XGBmodel.predict(X_train_resampled)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train_resampled, train_predictions)\n",
    "\n",
    "eval_predictions = XGBmodel.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided f1_score values, it appears that the model is likely overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Model #005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes model\n",
    "# Train the model on the resampled training data\n",
    "nb_model.fit(X_train_resampled,y_train_resampled)\n",
    "#make predictions on validation set\n",
    "nb_preds = nb_model.predict(X_eval_df)\n",
    "\n",
    "nb_f1Score = f1_score(y_eval, nb_preds)\n",
    "nb_f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, nb_preds)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "nb_auc_score = roc_auc_score(y_eval, nb_preds)\n",
    "nb_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % nb_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_predictions = nb_model.predict(X_train_resampled)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train_resampled, train_predictions)\n",
    "\n",
    "eval_predictions = nb_model.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided f1_score values, it appears that the model is overfitting the training data. The f1_score on the training data (0.7715) is considerably higher than the f1_score on the test data (0.5185), indicating that the model is not generalizing well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Model #006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the train dataset\n",
    "knn.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make prediction on the validation datset\n",
    "val_pred =knn.predict(X_eval_df)\n",
    "\n",
    "kn_f1score = f1_score(y_eval, val_pred)\n",
    "kn_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, val_pred)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "kn_auc_score = roc_auc_score(y_eval, val_pred)\n",
    "kn_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % kn_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_predictions = knn.predict(X_train_resampled)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train_resampled, train_predictions)\n",
    "\n",
    "eval_predictions = knn.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided f1_score values, it appears that the model is overfitting the training data. The f1_score on the training data (0.9151) is considerably higher than the f1_score on the test data (0.6407), indicating that the model is not generalizing well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SVM classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train th model using the training data\n",
    "svc.fit(X_train_df, y_train)\n",
    "\n",
    "# make predictions on validation set\n",
    "svcpred = svc.predict(X_eval_df)\n",
    "\n",
    "svc_f1Score =f1_score(y_eval, svcpred)\n",
    "svc_f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, svcpred)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "svc_auc_score = roc_auc_score(y_eval, svcpred)\n",
    "svc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % svc_auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on training set\n",
    "train_pred = knn.predict(X_train_df)\n",
    "# training accuracy\n",
    "train_score = f1_score(y_train, train_pred)\n",
    "\n",
    "\n",
    "# make prediction on unseen data\n",
    "eval_pred = knn.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_pred)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S73ZUqkUmyz_"
   },
   "source": [
    "## Models comparison\n",
    "Create a pandas dataframe that will allow you to compare your models.\n",
    "\n",
    "\n",
    "\n",
    "You might use the pandas dataframe method `.sort_values()` to sort the dataframe regarding the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'model': ['Logistic Regression','Decision Tree', 'Random Forest', 'XGBoost', 'Naive Bayes', 'K Neighbor','SVC'],\n",
    "          'f1_score':[lr_f1Score,dt_f1score,rf_f1Score,xg_f1Score,nb_f1Score,kn_f1score, svc_f1Score],\n",
    "          'AUC_score':[lr_auc_score, dt_auc_score, rf_auc_score, xg_auc_score, nb_auc_score, kn_auc_score, svc_auc_score]}\n",
    "comp_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort the DataFrame by fi_score in descending order\n",
    "df_sorted = comp_df.sort_values(by=['f1_score','AUC_score'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "# print the sorted DataFrame\n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.plot(kind='bar',figsize=(8,8),x ='model',title=\"Model Comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezmrABneph31"
   },
   "source": [
    "## Hyperparameters tuning \n",
    "\n",
    "Fine-tune the Top-k models (3 < k < 5) using a ` GridSearchCV`  (that is in sklearn.model_selection\n",
    ") to find the best hyperparameters and achieve the maximum performance of each of the Top-k models, then compare them again to select the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refit\n",
    "\n",
    "The refit parameter in GridSearchCV allows the model to be retrained on the entire dataset using the best hyperparameters found during grid search. This way, the model can have the best possible performance on new, unseen data. In other words, performing refit on hyperparameter tuning mainly suggests that after searching fro the optimal hyperparameters using a validation set, you train the final model on the entire training set using those paramters. This can improve the model's performance on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a scoring dictionary with 'f1' 'roc_auc' as scoring metrics\n",
    "scoring = {'F1': 'f1', 'ROC AUC': 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpTEDK9RrlRl"
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'C': [10,30,50,70,80,100] ,  # Inverse of regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Regularization penalty\n",
    "    'solver': ['liblinear', 'saga'],  # Solver algorithm for optimization\n",
    "    'max_iter': [10,20,30,40,50]  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Create a K Neighbors model\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "lr_grid_search = GridSearchCV(Lrmodel, param_grid, scoring=scoring,cv=5, refit='F1')\n",
    "lr_grid_search.fit(X_train_df, y_train)\n",
    "\n",
    "# Print the best hyperparameter and men cross_validation score for F1 and ROC AUC\n",
    "print('Best hyperparameters:', lr_grid_search.best_params_)\n",
    "print('Best estimator:', lr_grid_search.best_estimator_)\n",
    "print('Best Lr_F1score:', lr_grid_search.best_score_)\n",
    "print('Best Lr_auc_score:', lr_grid_search.cv_results_['mean_test_ROC AUC'][lr_grid_search.best_index_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters to tune\n",
    "dt_param= {\n",
    "    'max_depth': [5,10,15] ,\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'min_samples_leaf': [1,2,4], \n",
    "    'max_features': [5,10,15]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "dt_grid_search = GridSearchCV(dt_model, dt_param, scoring=scoring,cv=5, refit='F1')\n",
    "dt_grid_search.fit(X_train_df, y_train)\n",
    "\n",
    "# Print the best hyperparameter and men cross_validation score for F1 and ROC AUC\n",
    "print('Best hyperparameters:', dt_grid_search.best_params_)\n",
    "print('Best estimator:', dt_grid_search.best_estimator_)\n",
    "print('Best dt_F1score:', dt_grid_search.best_score_)\n",
    "print('Best dt_auc_score:', dt_grid_search.cv_results_['mean_test_ROC AUC'][dt_grid_search.best_index_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_param = {\n",
    "    'n_estimators' : [10,20,30,40,50],\n",
    "    'max_depth': [5,10,15,20,25],\n",
    "    'min_samples_split': [2,5,10,15,20],\n",
    "    'min_samples_leaf': [1,2,4],\n",
    "    'max_features': [1,3,5,8,10],\n",
    "    'bootstrap': [True,False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the evaluation set using the best model\n",
    "lr_preds = Lrmodel.predict(X_eval_df)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1Score = f1_score(y_eval, lr_preds)\n",
    "f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, lr_preds)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "auc_score = roc_auc_score(y_eval, lr_preds)\n",
    "auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate predictions on training set\n",
    "train_predictions = Lrmodel.predict(X_train_resampled)\n",
    "# training accuracy\n",
    "train_score= f1_score(y_train_resampled,train_predictions)\n",
    "\n",
    "eval_predictions = Lrmodel.predict(X_eval_df)\n",
    "eval_score = f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided f1_score values, it appears that the model is likely overfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'C': [10,30,50,70,80,100],  # Inverse of regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Regularization penalty\n",
    "    'solver': ['liblinear', 'saga'],  # Solver algorithm for optimization\n",
    "    'max_iter': [10,20,30,40,50]  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Create a logistic regression model\n",
    "#lr_model = LogisticRegression()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(Lrmodel, param_grid, scoring='f1')\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best logistic regression model\n",
    "best_lr_model = grid_search.best_estimator_\n",
    "best_lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the evaluation set using the best model\n",
    "lrpreds = Lrmodel.predict(X_eval_df)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1Score = f1_score(y_eval, lrpreds)\n",
    "f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the false positive rate, true positive rate, and thresholds using roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_eval, lrpreds)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve) using roc_auc_score\n",
    "auc_score = roc_auc_score(y_eval, lrpreds)\n",
    "auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate predictions on training set\n",
    "train_predictions = Lrmodel.predict(X_train_resampled)\n",
    "# training accuracy\n",
    "train_score =f1_score(y_train_resampled, train_predictions)\n",
    "\n",
    "eval_predictions = Lrmodel.predict(X_eval)\n",
    "eval_score= f1_score(y_eval, eval_predictions)\n",
    "print(f\"f1_score on train score {train_score}\")\n",
    "print(f\"f1_score on test score {eval_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided f1_score values, it appears that the model is likely overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_7_gsrX2VSk"
   },
   "source": [
    "# Export key components\n",
    "Here is the section to **export** the important ML objects that will be use to develop an app: *Encoder, Scaler, ColumnTransformer, Model, Pipeline, etc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32mviWyGAMqP"
   },
   "outputs": [],
   "source": [
    "#creating a file path to save all the componets in.\n",
    "if not os.path.exists(\"key_comp\"):\n",
    "    os.makedirs(\"key_comp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the destination path to the \"export\" directory\n",
    "destination = os.path.join(\".\", \"key_comp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_clf = {\n",
    "    \"num_imputer\":numerical_imputer,\n",
    "    \"scaler\": scaler,\n",
    "    \"models\": Lrmodel \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the KNN model\n",
    "with open(os.path.join(destination, \"Lrmodel.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(components_clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list --format=freeze >key_comp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "S73ZUqkUmyz_"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
